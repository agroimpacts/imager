---
title: "planet_downloading"
author: "Lei Song"
date: "12/2/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

All the scripts, files and folders are under `planet`.

## Set yamls

The yaml files are saved in `cfg`:

1. aws_cred.yml is the the **yml** file with AWS S3 credentials needed by `porder` to deliver data. Set all values accordlingly.
2. config.yaml is the **yaml** file with a lot of parameters. This file is needed by all the scripts to download images. Set all values carefully. _Make sure the common tables with `labeller` have the same name here_.

## Run scripts

1. Run `align_grid_tile.R` to get the table of pairs of grid and tile for the study area. This script just need to run once, then the file will be saved out to `catalog` folder, which can be used later.

2. Run `generate_catalog_before_rf.R` to to extract all images in S3 and then gather all the information into planet_catalog table, but without tms_url for now.

3. Run `fill_scenes_data.R` to to register scenes to Raster Foundary and build projects for the whole study area, theninsert the tms_url into planet_catalog csv generated by `generate_catalog_before_rf.R`. The table is ready to use for now.

## Other scrips

There are some modified old scripts might be useful:

1. `rasterfoundry_register_csv.py` to register images based on a csv file. It might be useful to register the last probability maps.
2. `rasterfoundry_register_scene.py` to register one scene to raster Foundary.
3. `rasterfoundry_register_each.py` to build one project to raster Foundary.
