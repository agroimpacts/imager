{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import gdal\n",
    "import geopandas as gpd\n",
    "import osr\n",
    "from shapely.geometry import mapping\n",
    "from math import ceil\n",
    "from datetime import datetime\n",
    "import os\n",
    "import click\n",
    "from scipy import ndimage\n",
    "import logging\n",
    "import time\n",
    "import subprocess\n",
    "import yaml\n",
    "from pytz import timezone\n",
    "\n",
    "def get_matching_s3_keys(bucket, prefix='', suffix=''):\n",
    "    \"\"\"\n",
    "    Generate the keys in an S3 bucket.\n",
    "    arg:\n",
    "        bucket: Name of the S3 bucket.\n",
    "        prefix: Only fetch keys that start with this prefix (optional).\n",
    "        suffix: Only fetch keys that end with this suffix (optional).\n",
    "    return:\n",
    "        (string) key\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    kwargs = {'Bucket': bucket}\n",
    "\n",
    "    # If the prefix is a single string (not a tuple of strings), we can\n",
    "    # do the filtering directly in the S3 API.\n",
    "    if isinstance(prefix, str):\n",
    "        kwargs['Prefix'] = prefix\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # The S3 API response is a large blob of metadata.\n",
    "        # 'Contents' contains information about the listed objects.\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "        for obj in resp['Contents']:\n",
    "            key = obj['Key']\n",
    "            if key.startswith(prefix) and key.endswith(suffix):\n",
    "                return key\n",
    "\n",
    "        # The S3 API is paginated, returning up to 1000 keys at a time.\n",
    "        # Pass the continuation token into the next response, until we\n",
    "        # reach the final page (when this field is missing).\n",
    "        try:\n",
    "            kwargs['ContinuationToken'] = resp['NextContinuationToken']\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "def get_img_fullpath(bucket, prefix='', suffix=''):\n",
    "    \"\"\"\n",
    "    Generate the keys in an S3 bucket.\n",
    "    arg:\n",
    "        bucket: Name of the S3 bucket.\n",
    "        prefix: Only fetch keys that start with this prefix (optional).\n",
    "        suffix: Only fetch keys that end with this suffix (optional).\n",
    "    return:\n",
    "        (string) key\n",
    "    \"\"\"\n",
    "\n",
    "def get_geojson_pcs(bucket, gjs_tile, prefix,  img_folder, sample_img_nm, img_fullpth_catalog, logger):\n",
    "    \"\"\"\n",
    "    covert geojson of a tile to pcs of sample image.\n",
    "    arg:\n",
    "        bucket: Name of the S3 bucket.\n",
    "        gjs_tile: geopandas object\n",
    "        prefix: the prefix for tile_folder and img_folder\n",
    "        tile_folder: the folder name for storing tile geojson\n",
    "        img_folder: the folder name for storing image geojson\n",
    "        sample_img_nm: the name of sample image used to extract pcs\n",
    "        logger: logger\n",
    "    return:\n",
    "        'extent_geojson' sharply geojson object\n",
    "        'proj' projection object\n",
    "    \"\"\"\n",
    "\n",
    "    # read projection from sample planet image\n",
    "    prefix_x = \"{}/{}\".format(prefix, img_folder)\n",
    "    #sub_img_pth = get_matching_s3_keys(bucket, prefix=prefix_x, suffix=\"{}_3B_AnalyticMS_SR.tif\".format(sample_img_nm))\n",
    "    s = img_fullpth_catalog.stack() # convert entire data frame into a series of values\n",
    "    sub_img_pth = img_fullpth_catalog.iloc[s[s.str.contains(sample_img_nm,na=False)].index.get_level_values(0)].values[0][0]\n",
    "    uri_img_gdal = \"/vsis3/{}/{}\".format(bucket, sub_img_pth)\n",
    "    img = gdal.Open(uri_img_gdal)\n",
    "    if img is None:\n",
    "        logger.error(\"reading {} failed\". format(uri_img_gdal))\n",
    "\n",
    "    # convert tile to planet image pcs\n",
    "    gjs_tile_pcs = gjs_tile.to_crs(epsg=osr.SpatialReference(wkt=img.GetProjection()).GetAttrValue('AUTHORITY',1))\n",
    "    extent_geojson = mapping(gjs_tile_pcs['geometry'])\n",
    "    proj = img.GetProjectionRef()\n",
    "    img = None\n",
    "    return extent_geojson, proj\n",
    "\n",
    "\n",
    "def get_extent(extent_geojson, res):\n",
    "    \"\"\"\n",
    "    read geojson of a tile from an S3 bucket, and convert projection to sample image.\n",
    "    arg:\n",
    "        'extent_geojson': sharply geojson object\n",
    "        res: planet resolution\n",
    "    return:\n",
    "        (float, float, float, float), (int, int)) tuple\n",
    "    \"\"\"\n",
    "    # txmin = min([row[0] for row in extent_geojson['coordinates'][0]]) - res / 2.0\n",
    "    # txmax = max([row[0] for row in extent_geojson['coordinates'][0]]) + res / 2.0\n",
    "    # tymin = min([row[1] for row in extent_geojson['coordinates'][0]]) - res / 2.0\n",
    "    # tymax = max([row[1] for row in extent_geojson['coordinates'][0]]) + res / 2.0\n",
    "    txmin = extent_geojson['bbox'][0] - res / 2.0\n",
    "    txmax = extent_geojson['bbox'][2] + res / 2.0\n",
    "    tymin = extent_geojson['bbox'][1] - res / 2.0\n",
    "    tymax = extent_geojson['bbox'][3] + res / 2.0\n",
    "    n_row = ceil((tymax - tymin)/res)\n",
    "    n_col = ceil((txmax - txmin)/res)\n",
    "    txmin_new = (txmin + txmax)/2 - n_row / 2 * res\n",
    "    txmax_new = (txmin + txmax)/2 + n_row / 2 * res\n",
    "    tymin_new = (tymin + tymax)/2 - n_col / 2 * res\n",
    "    tymax_new = (tymin + tymax)/2 + n_col / 2 * res\n",
    "    return (txmin_new, txmax_new, tymin_new, tymax_new), (n_row, n_col)\n",
    "\n",
    "def parse_yaml_from_s3(bucket, prefix):\n",
    "    \"\"\"\n",
    "    read bucket, prefix from yaml.\n",
    "    arg:\n",
    "        bucket: Name of the S3 bucket.\n",
    "        prefix: the name for yaml file\n",
    "    return:\n",
    "        yaml object\n",
    "    \"\"\"\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Bucket(bucket).Object(prefix).get()['Body'].read()\n",
    "    return yaml.load(obj)\n",
    "\n",
    "\n",
    "def parse_catalog_from_s3(bucket, prefix, catalog_name):\n",
    "    \"\"\"\n",
    "    read bucket, prefix from yaml.\n",
    "    arg:\n",
    "        bucket: Name of the S3 bucket.\n",
    "        prefix: prefix for yaml file\n",
    "        catalog_name: name of catalog file\n",
    "    return:\n",
    "        'catalog' pandas object\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    obj = s3.get_object(Bucket=bucket, Key='{}/{}'.format(prefix, catalog_name))\n",
    "    catalog = pd.read_csv(obj['Body'], sep=\" \")\n",
    "    return catalog\n",
    "\n",
    "\n",
    "def ard_generation(sub_catalog, bucket, prefix, aoi, tile_id, img_folder, proj, bounds, n_row, n_col, tmp_pth, logger,\n",
    "                   dry_lower_ordinal, dry_upper_ordinal, wet_lower_ordinal, wet_upper_ordinal):\n",
    "    \"\"\"\n",
    "    generate ARD image per image in sub catalog.\n",
    "    arg:\n",
    "        sub_catalog: a table recording all images for tile_id\n",
    "        bucket: Name of the S3 bucket.\n",
    "        prefix: the prefix for tile_folder and img_folder\n",
    "        aoi: aoi to be processed\n",
    "        tile_id: id of current tile to be processed\n",
    "        img_folder: the folder name for storing image geojson\n",
    "        proj: the projection of outputted ARD\n",
    "        bounds: xmin, xmax, ymin, ymax defining the extent of ard\n",
    "        tmp_path: tmp path defining the path for storing temporal files\n",
    "        logger: logging object\n",
    "        dry_lower_ordinal: lower bounds of ordinal days for dry season\n",
    "        dry_upper_ordinal: upper bounds of ordinal days for dry season\n",
    "        wet_lower_ordinal: lower bounds of ordinal days for wet season\n",
    "        wet_upper_ordinal: upper bounds of ordinal days for wet season\n",
    "    return:\n",
    "        'extent_geojson' sharply geojson object\n",
    "    \"\"\"\n",
    "    # initialize a record list for clear observations for each days\n",
    "    clear_records = [0] * 366\n",
    "    imgname_records = [0] * 366\n",
    "\n",
    "    tile_folder = os.path.join(tmp_pth, 'aoi{}_tile{}'.format(aoi, tile_id))\n",
    "    if not os.path.exists(tile_folder):\n",
    "        os.mkdir(tile_folder)\n",
    "\n",
    "    s = img_fullpth_catalog.stack() # convert entire data frame into a series of values\n",
    "    ###############################################\n",
    "    for i in range(len(sub_catalog)):\n",
    "        img_name = sub_catalog.iloc[i,0]\n",
    "        prefix_x = \"{}/{}\".format(prefix, img_folder)\n",
    "        # sub_img_name = get_matching_s3_keys(bucket, prefix=prefix_x, suffix=\"{}_3B_AnalyticMS_SR.tif\".format(img_name))\n",
    "        sub_img_pth = img_fullpth_catalog.iloc[s[s.str.contains(sample_img_nm,na=False)].index.get_level_values(0)].values[0][0]\n",
    "        if sub_img_pth is None:\n",
    "            continue\n",
    "        sub_msk_pth = sub_img_pth.replace('AnalyticMS_SR', 'AnalyticMS_DN_udm')\n",
    "\n",
    "        # note that gdal and rasterio uri formats are different\n",
    "        # uri_img = \"s3://{}/{}\".format(bucket, sub_img_pth)\n",
    "        uri_img_gdal = \"/vsis3/{}/{}\".format(bucket, sub_img_pth)\n",
    "        # uri_msk = \"s3://{}/{}\".format(bucket, sub_msk_name)\n",
    "        uri_msk_gdal = \"/vsis3/{}/{}\".format(bucket, sub_msk_pth)\n",
    "\n",
    "        ordinal_dates = datetime.strptime(img_name[0:8], '%Y%m%d').date().toordinal()\n",
    "        if ordinal_dates not in range(dry_lower_ordinal, dry_upper_ordinal + 1) and ordinal_dates \\\n",
    "                not in range(wet_lower_ordinal, wet_upper_ordinal + 1):\n",
    "            continue\n",
    "\n",
    "        doy = datetime.strptime(img_name[0:8], '%Y%m%d').date().timetuple().tm_yday\n",
    "\n",
    "        outname = \"PLANET%s%s\" % (str(datetime.strptime(img_name[0:8], '%Y%m%d').date().year),\n",
    "                                  str(\"{0:0=3d}\".format(doy)))\n",
    "\n",
    "        img = gdal.Open(uri_img_gdal)\n",
    "        msk = gdal.Open(uri_msk_gdal)\n",
    "\n",
    "        # partial dataset, skip\n",
    "        if img is None:\n",
    "            logger.warn(\"couldn't find {} from s3\".format(uri_img_gdal))\n",
    "            continue\n",
    "\n",
    "        if msk is None:\n",
    "            logger.warn(\"couldn't find {} from s3\".format(uri_msk_gdal))\n",
    "            continue\n",
    "\n",
    "        out_img = gdal.Warp(os.path.join(tile_folder, '_tmp_img'), img, outputBounds=[bounds[0], bounds[2], bounds[1],\n",
    "                                                                                  bounds[3]],\n",
    "                            width=n_col, height=n_row, dstNodata=-9999, outputType=gdal.GDT_Int16, dstSRS=proj)\n",
    "        out_msk = gdal.Warp(os.path.join(tile_folder, '_tmp_msk'), msk, outputBounds=[bounds[0], bounds[2], bounds[1],\n",
    "                                                                                  bounds[3]], width=n_col, height=n_row,\n",
    "                            dstNodata=-9999, outputType=gdal.GDT_Int16, dstSRS=proj)\n",
    "\n",
    "        n_valid_pixels = len(out_img.GetRasterBand(1).ReadAsArray()[out_img.GetRasterBand(1).ReadAsArray() > -9999])\n",
    "        n_clear_pixels = len(out_msk.GetRasterBand(1).ReadAsArray()[out_msk.GetRasterBand(1).ReadAsArray() == 0])\n",
    "\n",
    "        # firstly, see if clear observation is more than the record; if not, not necessary to process\n",
    "        if n_clear_pixels < clear_records[doy-1]:\n",
    "            continue\n",
    "        else:\n",
    "            if n_clear_pixels/n_valid_pixels > 0.2:\n",
    "\n",
    "                # if already created, delete old files\n",
    "                if clear_records[doy-1] > 0:\n",
    "                    os.remove(os.path.join(tile_folder, imgname_records[doy-1]))\n",
    "                    os.remove(os.path.join(tile_folder, imgname_records[doy-1]+'.hdr'))\n",
    "\n",
    "                out_img_b1_med = ndimage.median_filter(out_img.GetRasterBand(1).ReadAsArray(), size=3)\n",
    "                out_img_b2_med = ndimage.median_filter(out_img.GetRasterBand(2).ReadAsArray(), size=3)\n",
    "                out_img_b3_med = ndimage.median_filter(out_img.GetRasterBand(3).ReadAsArray(), size=3)\n",
    "                out_img_b4_med = ndimage.median_filter(out_img.GetRasterBand(4).ReadAsArray(), size=3)\n",
    "\n",
    "                clear_records[doy-1] = n_clear_pixels\n",
    "                imgname_records[doy-1] = outname + img_name[8:len(img_name)]\n",
    "                outdriver1 = gdal.GetDriverByName(\"ENVI\")\n",
    "                outdata = outdriver1.Create(os.path.join(tile_folder,\n",
    "                                                         outname+img_name[8:len(img_name)]),\n",
    "                                            n_col, n_row, 5, gdal.GDT_Int16, options=[\"INTERLEAVE=BIP\"])\n",
    "                outdata.GetRasterBand(1).WriteArray(out_img_b1_med)\n",
    "                outdata.FlushCache()\n",
    "                outdata.GetRasterBand(2).WriteArray(out_img_b2_med)\n",
    "                outdata.FlushCache()\n",
    "                outdata.GetRasterBand(3).WriteArray(out_img_b3_med)\n",
    "                outdata.FlushCache()\n",
    "                outdata.GetRasterBand(4).WriteArray(out_img_b4_med)\n",
    "                outdata.FlushCache()\n",
    "                outdata.GetRasterBand(5).WriteArray(out_msk.GetRasterBand(1).ReadAsArray())\n",
    "                outdata.FlushCache()\n",
    "\n",
    "                outdata.SetGeoTransform(out_img.GetGeoTransform())\n",
    "                outdata.FlushCache()\n",
    "                outdata.SetProjection(proj)\n",
    "                outdata.FlushCache()\n",
    "\n",
    "                del outdata\n",
    "\n",
    "        img = None\n",
    "        msk = None\n",
    "\n",
    "def composite_generation(compositing_exe_path, bucket, prefix,  gjs_tile, tile_id, img_folder, tmp_pth,\n",
    "                         logger, dry_lower_ordinal, dry_upper_ordinal, wet_lower_ordinal, wet_upper_ordinal):\n",
    "    \"\"\"\n",
    "    generate composite image in sub catalog.\n",
    "    arg:\n",
    "        compositing_exe_path: directory for compositing exe\n",
    "        gjs_tile: geopandas vector file for a targeted tile, using GCS system\n",
    "        tile_id: id of current tile to be processed\n",
    "        img_folder: the folder name for storing image geojson\n",
    "        tmp_pth: the outputted folder for pcs and gcs images\n",
    "        logger: logging object\n",
    "        dry_lower_ordinal: lower bounds of ordinal days for dry season\n",
    "        dry_upper_ordinal: upper bounds of ordinal days for dry season\n",
    "        wet_lower_ordinal: lower bounds of ordinal days for wet season\n",
    "        wet_upper_ordinal: upper bounds of ordinal days for wet season\n",
    "    \"\"\"\n",
    "\n",
    "    # fetch tile info, which will be used to crop intermediate compositing image\n",
    "    extent_geojson_gcs = mapping(gjs_tile['geometry'][0])\n",
    "    txmin = min([row[0] for row in extent_geojson_gcs['coordinates'][0]])\n",
    "    txmax = max([row[0] for row in extent_geojson_gcs['coordinates'][0]])\n",
    "    tymin = min([row[1] for row in extent_geojson_gcs['coordinates'][0]])\n",
    "    tymax = max([row[1] for row in extent_geojson_gcs['coordinates'][0]])\n",
    "\n",
    "    # compositing dry season\n",
    "    out_path_pcs_dry = os.path.join(tmp_pth, 'tile{}_{}_{}_pcs.tif'.format(tile_id, dry_lower_ordinal, dry_lower_ordinal))\n",
    "    cmd = [compositing_exe_path, img_folder, out_path_pcs_dry, tile_id, dry_lower_ordinal, dry_upper_ordinal]\n",
    "    # run composite exe\n",
    "    try:\n",
    "        p = subprocess.check_output(cmd, stderr=subprocess.STDOUT)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.error(\"compositing error for tile {} at dry season: {}\".format(tile_id, e))\n",
    "\n",
    "    # reproject and crop compositing image to align with GCS tile system\n",
    "    out_path_gcs_dry = os.path.join(tmp_pth, 'tile{}_{}_{}.tif'.format(tile_id, dry_lower_ordinal, dry_lower_ordinal))\n",
    "    img = gdal.Open(out_path_pcs_dry)\n",
    "    if img is None:\n",
    "        logger.error(\"couldn't find pcs-based compositing result for tile {}\".format(tile_id))\n",
    "\n",
    "    out_img = gdal.Warp(out_path_gcs_dry, img, outputBounds=[txmin, tymin, txmax, tymax], resampleAlg=gdal.GRA_Bilinear, width=2000,\n",
    "                        height=2000, dstNodata=-9999, outputType=gdal.GDT_Int16, dstSRS='EPSG:4326')\n",
    "\n",
    "    # release memory\n",
    "    out_img = None\n",
    "    img = None\n",
    "\n",
    "    # compositing wet season\n",
    "    out_path_pcs_wet = os.path.join(tmp_pth, 'tile{}_{}_{}_pcs.tif'.format(tile_id, wet_lower_ordinal, wet_lower_ordinal))\n",
    "    cmd = [compositing_exe_path, img_folder, out_path_pcs_wet, tile_id, wet_lower_ordinal, wet_upper_ordinal]\n",
    "    # run composite exe\n",
    "    try:\n",
    "        p = subprocess.check_output(cmd, stderr=subprocess.STDOUT)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.error(\"compositing error for tile {} at wet season: {}\".format(tile_id, e))\n",
    "\n",
    "    # reproject and crop compositing image to align with GCS tile system\n",
    "    out_path_gcs_wet = os.path.join(tmp_pth, 'tile{}_{}_{}.tif'.format(tile_id, wet_lower_ordinal, wet_lower_ordinal))\n",
    "    img = gdal.Open(out_path_pcs_wet)\n",
    "    if img is None:\n",
    "        logger.error(\"couldn't find pcs-based compositing result for tile {}\".format(tile_id))\n",
    "\n",
    "    out_img = gdal.Warp(out_path_gcs_wet, img, outputBounds=[txmin, tymin, txmax, tymax], resampleAlg=gdal.GRA_Bilinear, width=2000,\n",
    "                        height=2000, dstNodata=-9999, outputType=gdal.GDT_Int16, dstSRS='EPSG:4326')\n",
    "\n",
    "    out_img = None\n",
    "    img = None\n",
    "\n",
    "\n",
    "    # # upload compositing image to s3\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.upload_file(out_path_gcs_dry, bucket, '{}/composite_sr/OS/{}'.format(prefix, out_path_gcs_dry))\n",
    "    s3.upload_file(out_path_gcs_wet, bucket, '{}/composite_sr/GS/{}'.format(prefix, out_path_gcs_wet))\n",
    "\n",
    "    # delete local composte files\n",
    "    os.remove(os.path.join(tmp_pth, out_path_gcs_dry))\n",
    "    os.remove(os.path.join(tmp_pth, out_path_pcs_dry))\n",
    "    os.remove(os.path.join(tmp_pth, out_path_gcs_wet))\n",
    "    os.remove(os.path.join(tmp_pth, out_path_pcs_wet))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/composite/lib/python3.7/site-packages/ipykernel_launcher.py:135: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n"
     ]
    }
   ],
   "source": [
    "out_folder = '/tmp/'\n",
    "s3_bucket = '***REMOVED***'\n",
    "config_filename = 'cvmapper_config_composite.yaml'\n",
    "tile_id = 615672\n",
    "\n",
    "res = 3\n",
    "compositing_exe_path = '/home/ubuntu/imager/C/AFMapTSComposite/bin/composite'\n",
    "log_path = '%s/log/planet_composite.log' % os.environ['HOME']\n",
    "logging.basicConfig(filename=log_path, filemode='w', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "s3 = boto3.resource('s3')\n",
    "out_folder = 'tmp'\n",
    "params = parse_yaml_from_s3(s3_bucket, config_filename)['mapper']\n",
    "\n",
    "#read individual parameters\n",
    "prefix = params['prefix']\n",
    "# img_catalog links images and tiles\n",
    "img_catalog_name = params['img_catalog_name']\n",
    "img_catalog_pth = params['img_catalog_pth']\n",
    "img_all_folder = params['img_all_folder']\n",
    "tiles_geojson_path = params['tile_geojson_path']\n",
    "ard_folder = params['ard_folder']\n",
    "aoi = params['aoi']\n",
    "\n",
    "# define lower and upper bounds for dry and wet season\n",
    "# 2017/12/01\n",
    "dry_lower_ordinal = params['dry_lower_ordinal']\n",
    "# 2018/02/28\n",
    "dry_upper_ordinal = params['dry_upper_ordinal']\n",
    "# 2018/05/01\n",
    "wet_lower_ordinal = params['wet_lower_ordinal']\n",
    "# 2018/09/30\n",
    "wet_upper_ordinal = params['wet_upper_ordinal']\n",
    "\n",
    "# time zone\n",
    "tz = timezone('US/Eastern')\n",
    "logger.info(\"starting making planet ARD images: {} for aoi_{}\".format(datetime.now(tz).strftime('%Y-%m-%d %H:%M:%S'), aoi))\n",
    "\n",
    "# fetch tile catalog\n",
    "img_catalog = parse_catalog_from_s3(s3_bucket, prefix, img_catalog_name)\n",
    "img_fullpth_catalog = parse_catalog_from_s3(s3_bucket, prefix, img_catalog_pth)\n",
    "uri_tile = \"s3://{}/{}/{}\".format(s3_bucket, prefix, tiles_geojson_path)\n",
    "gjs_tile = gpd.read_file(uri_tile)\n",
    "if gjs_tile is None:\n",
    "    logger.error(\"reading {} failed\". format(uri_tile))\n",
    "    \n",
    "foc_img_catalog = img_catalog.loc[img_catalog['tile'] == int(tile_id)]\n",
    "sample_img_nm = foc_img_catalog.iloc[0, 0]\n",
    "tile_geojson, proj = get_geojson_pcs(s3_bucket, gjs_tile[gjs_tile['tile'] == int(tile_id)], prefix, img_all_folder, sample_img_nm, \n",
    "                                     img_fullpth_catalog, logger)\n",
    "bounds, (n_row, n_col) = get_extent(tile_geojson, res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_pth = '/tmp'\n",
    "sub_catalog = foc_img_catalog\n",
    "clear_records = [0] * 366\n",
    "imgname_records = [0] * 366\n",
    "img_folder = img_all_folder\n",
    "bucket = s3_bucket\n",
    "\n",
    "\n",
    "tile_folder = os.path.join(tmp_pth, 'aoi{}_tile{}'.format(aoi, tile_id))\n",
    "if not os.path.exists(tile_folder):\n",
    "    os.mkdir(tile_folder)\n",
    "    \n",
    "s = img_fullpth_catalog.stack() # convert entire data frame into a series of values\n",
    "###############################################\n",
    "for i in range(len(sub_catalog)):\n",
    "    img_name = sub_catalog.iloc[i,0]\n",
    "    prefix_x = \"{}/{}\".format(prefix, img_folder)\n",
    "    # sub_img_name = get_matching_s3_keys(bucket, prefix=prefix_x, suffix=\"{}_3B_AnalyticMS_SR.tif\".format(img_name))\n",
    "    sub_img_pth = img_fullpth_catalog.iloc[s[s.str.contains(sample_img_nm,na=False)].index.get_level_values(0)].values[0][0]\n",
    "    if sub_img_pth is None:\n",
    "        continue\n",
    "    sub_msk_pth = sub_img_pth.replace('AnalyticMS_SR', 'AnalyticMS_DN_udm')\n",
    "\n",
    "    # note that gdal and rasterio uri formats are different\n",
    "    # uri_img = \"s3://{}/{}\".format(bucket, sub_img_pth)\n",
    "    uri_img_gdal = \"/vsis3/{}/{}\".format(bucket, sub_img_pth)\n",
    "    # uri_msk = \"s3://{}/{}\".format(bucket, sub_msk_name)\n",
    "    uri_msk_gdal = \"/vsis3/{}/{}\".format(bucket, sub_msk_pth)\n",
    "\n",
    "    ordinal_dates = datetime.strptime(img_name[0:8], '%Y%m%d').date().toordinal()\n",
    "    if ordinal_dates not in range(dry_lower_ordinal, dry_upper_ordinal + 1) and ordinal_dates \\\n",
    "            not in range(wet_lower_ordinal, wet_upper_ordinal + 1):\n",
    "        continue\n",
    "\n",
    "    doy = datetime.strptime(img_name[0:8], '%Y%m%d').date().timetuple().tm_yday\n",
    "\n",
    "    outname = \"PLANET%s%s\" % (str(datetime.strptime(img_name[0:8], '%Y%m%d').date().year),\n",
    "                              str(\"{0:0=3d}\".format(doy)))\n",
    "\n",
    "    img = gdal.Open(uri_img_gdal)\n",
    "    msk = gdal.Open(uri_msk_gdal)\n",
    "\n",
    "    # partial dataset, skip\n",
    "    if img is None:\n",
    "        logger.warn(\"couldn't find {} from s3\".format(uri_img_gdal))\n",
    "        continue\n",
    "\n",
    "    if msk is None:\n",
    "        logger.warn(\"couldn't find {} from s3\".format(uri_msk_gdal))\n",
    "        continue\n",
    "\n",
    "    out_img = gdal.Warp(os.path.join(tile_folder, '_tmp_img'), img, outputBounds=[bounds[0], bounds[2], bounds[1],\n",
    "                                                                              bounds[3]],\n",
    "                        width=n_col, height=n_row, dstNodata=-9999, outputType=gdal.GDT_Int16, dstSRS=proj)\n",
    "    out_msk = gdal.Warp(os.path.join(tile_folder, '_tmp_msk'), msk, outputBounds=[bounds[0], bounds[2], bounds[1],\n",
    "                                                                              bounds[3]], width=n_col, height=n_row,\n",
    "                        dstNodata=-9999, outputType=gdal.GDT_Int16, dstSRS=proj)\n",
    "\n",
    "    n_valid_pixels = len(out_img.GetRasterBand(1).ReadAsArray()[out_img.GetRasterBand(1).ReadAsArray() > -9999])\n",
    "    n_clear_pixels = len(out_msk.GetRasterBand(1).ReadAsArray()[out_msk.GetRasterBand(1).ReadAsArray() == 0])\n",
    "\n",
    "    # firstly, see if clear observation is more than the record; if not, not necessary to process\n",
    "    if n_clear_pixels < clear_records[doy-1]:\n",
    "        continue\n",
    "    else:\n",
    "        if n_clear_pixels/n_valid_pixels > 0.2:\n",
    "\n",
    "            # if already created, delete old files\n",
    "            if clear_records[doy-1] > 0:\n",
    "                os.remove(os.path.join(tile_folder, imgname_records[doy-1]))\n",
    "                os.remove(os.path.join(tile_folder, imgname_records[doy-1]+'.hdr'))\n",
    "\n",
    "            out_img_b1_med = ndimage.median_filter(out_img.GetRasterBand(1).ReadAsArray(), size=3)\n",
    "            out_img_b2_med = ndimage.median_filter(out_img.GetRasterBand(2).ReadAsArray(), size=3)\n",
    "            out_img_b3_med = ndimage.median_filter(out_img.GetRasterBand(3).ReadAsArray(), size=3)\n",
    "            out_img_b4_med = ndimage.median_filter(out_img.GetRasterBand(4).ReadAsArray(), size=3)\n",
    "\n",
    "            clear_records[doy-1] = n_clear_pixels\n",
    "            imgname_records[doy-1] = outname + img_name[8:len(img_name)]\n",
    "            outdriver1 = gdal.GetDriverByName(\"ENVI\")\n",
    "            outdata = outdriver1.Create(os.path.join(tile_folder,\n",
    "                                                     outname+img_name[8:len(img_name)]),\n",
    "                                        n_col, n_row, 5, gdal.GDT_Int16, options=[\"INTERLEAVE=BIP\"])\n",
    "            outdata.GetRasterBand(1).WriteArray(out_img_b1_med)\n",
    "            outdata.FlushCache()\n",
    "            outdata.GetRasterBand(2).WriteArray(out_img_b2_med)\n",
    "            outdata.FlushCache()\n",
    "            outdata.GetRasterBand(3).WriteArray(out_img_b3_med)\n",
    "            outdata.FlushCache()\n",
    "            outdata.GetRasterBand(4).WriteArray(out_img_b4_med)\n",
    "            outdata.FlushCache()\n",
    "            outdata.GetRasterBand(5).WriteArray(out_msk.GetRasterBand(1).ReadAsArray())\n",
    "            outdata.FlushCache()\n",
    "\n",
    "            outdata.SetGeoTransform(out_img.GetGeoTransform())\n",
    "            outdata.FlushCache()\n",
    "            outdata.SetProjection(proj)\n",
    "            outdata.FlushCache()\n",
    "\n",
    "            del outdata\n",
    "\n",
    "    img = None\n",
    "    msk = None\n",
    "\n",
    "os.remove(os.path.join(tile_folder, '_tmp_img'))\n",
    "os.remove(os.path.join(tile_folder, '_tmp_msk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_pth = '/tmp'\n",
    "sub_catalog = foc_img_catalog\n",
    "clear_records = [0] * 366\n",
    "imgname_records = [0] * 366\n",
    "img_folder = img_all_folder\n",
    "bucket = s3_bucket"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:composite]",
   "language": "python",
   "name": "conda-env-composite-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
