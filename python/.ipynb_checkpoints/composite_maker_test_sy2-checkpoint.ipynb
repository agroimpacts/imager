{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module consisted of two steps: 1) make planet ARD images and 2) call AFMapTSComposite for making composites\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import gdal\n",
    "import geopandas as gpd\n",
    "import osr\n",
    "from shapely.geometry import mapping\n",
    "from math import ceil\n",
    "from datetime import datetime\n",
    "import os\n",
    "import click\n",
    "from scipy import ndimage\n",
    "import logging\n",
    "import time\n",
    "import yaml\n",
    "import subprocess\n",
    "from pytz import timezone\n",
    "import shutil\n",
    "\n",
    "# (this function has been abandoned in the current version,  cause the searching efficiency over s3 is low)\n",
    "def get_matching_s3_keys(bucket, prefix='', suffix=''):\n",
    "    \"\"\"\n",
    "    Generate the keys in an S3 bucket.\n",
    "    arg:\n",
    "        bucket: Name of the S3 bucket.\n",
    "        prefix: Only fetch keys that start with this prefix (optional).\n",
    "        suffix: Only fetch keys that end with this suffix (optional).\n",
    "    return:\n",
    "        (string) key\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    kwargs = {'Bucket': bucket}\n",
    "\n",
    "    # If the prefix is a single string (not a tuple of strings), we can\n",
    "    # do the filtering directly in the S3 API.\n",
    "    if isinstance(prefix, str):\n",
    "        kwargs['Prefix'] = prefix\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # The S3 API response is a large blob of metadata.\n",
    "        # 'Contents' contains information about the listed objects.\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "        for obj in resp['Contents']:\n",
    "            key = obj['Key']\n",
    "            if key.startswith(prefix) and key.endswith(suffix):\n",
    "                return key\n",
    "\n",
    "        # The S3 API is paginated, returning up to 1000 keys at a time.\n",
    "        # Pass the continuation token into the next response, until we\n",
    "        # reach the final page (when this field is missing).\n",
    "        try:\n",
    "            kwargs['ContinuationToken'] = resp['NextContinuationToken']\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "\n",
    "def get_geojson_pcs(bucket, gpd_tile, sample_img_nm, img_fullpth_catalog, logger):\n",
    "    \"\"\"\n",
    "    covert geojson of a tile to pcs of sample image.\n",
    "    arg:\n",
    "        bucket: Name of the S3 bucket.\n",
    "        gpd_tile: geopandas object\n",
    "        tile_folder: the folder name for storing tile geojson\n",
    "        sample_img_nm: the name of sample image used to extract pcs\n",
    "        logger: logger\n",
    "    return:\n",
    "        'extent_geojson' sharply geojson object\n",
    "        'proj' projection object\n",
    "    \"\"\"\n",
    "\n",
    "    # read projection from sample planet image\n",
    "    #sub_img_pth = get_matching_s3_keys(bucket, prefix=prefix_x, suffix=\"{}_3B_AnalyticMS_SR.tif\".format(sample_img_nm))\n",
    "    s = img_fullpth_catalog.stack() # convert entire data frame into a series of values\n",
    "    sub_img_pth = img_fullpth_catalog.iloc[s[s.str.contains(sample_img_nm,na=False)].index.get_level_values(0)].values[0][0]\n",
    "    uri_img_gdal = \"/vsis3/{}/{}\".format(bucket, sub_img_pth)\n",
    "    img = gdal.Open(uri_img_gdal)\n",
    "    if img is None:\n",
    "        logger.error(\"reading {} failed\". format(uri_img_gdal))\n",
    "\n",
    "    # convert tile to planet image pcs\n",
    "    gpd_tile_pcs = gpd_tile.to_crs(epsg=osr.SpatialReference(wkt=img.GetProjection()).GetAttrValue('AUTHORITY',1))\n",
    "    extent_geojson = mapping(gpd_tile_pcs['geometry'])\n",
    "    proj = img.GetProjectionRef()\n",
    "    img = None\n",
    "    return extent_geojson, proj\n",
    "\n",
    "\n",
    "def get_extent(extent_geojson, res):\n",
    "    \"\"\"\n",
    "    read geojson of a tile from an S3 bucket, and convert projection to be aligned with sample image.\n",
    "    arg:\n",
    "        'extent_geojson': sharply geojson object\n",
    "        res: planet resolution\n",
    "    return:\n",
    "        (float, float, float, float), (int, int)) tuple\n",
    "    \"\"\"\n",
    "    # txmin = min([row[0] for row in extent_geojson['coordinates'][0]]) - res / 2.0\n",
    "    # txmax = max([row[0] for row in extent_geojson['coordinates'][0]]) + res / 2.0\n",
    "    # tymin = min([row[1] for row in extent_geojson['coordinates'][0]]) - res / 2.0\n",
    "    # tymax = max([row[1] for row in extent_geojson['coordinates'][0]]) + res / 2.0\n",
    "    txmin = extent_geojson['bbox'][0] - res / 2.0\n",
    "    txmax = extent_geojson['bbox'][2] + res / 2.0\n",
    "    tymin = extent_geojson['bbox'][1] - res / 2.0\n",
    "    tymax = extent_geojson['bbox'][3] + res / 2.0\n",
    "    n_row = ceil((tymax - tymin)/res)\n",
    "    n_col = ceil((txmax - txmin)/res)\n",
    "    txmin_new = (txmin + txmax)/2 - n_row / 2 * res\n",
    "    txmax_new = (txmin + txmax)/2 + n_row / 2 * res\n",
    "    tymin_new = (tymin + tymax)/2 - n_col / 2 * res\n",
    "    tymax_new = (tymin + tymax)/2 + n_col / 2 * res\n",
    "    return (txmin_new, txmax_new, tymin_new, tymax_new), (n_row, n_col)\n",
    "\n",
    "\n",
    "def parse_yaml_from_s3(bucket, prefix):\n",
    "    \"\"\"\n",
    "    read bucket, prefix from yaml.\n",
    "    arg:\n",
    "        bucket: Name of the S3 bucket.\n",
    "        prefix: the name for yaml file\n",
    "    return:\n",
    "        yaml object\n",
    "    \"\"\"\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Bucket(bucket).Object(prefix).get()['Body'].read()\n",
    "    return yaml.load(obj)\n",
    "\n",
    "\n",
    "def parse_catalog_from_s3(bucket, prefix, catalog_name):\n",
    "    \"\"\"\n",
    "    read bucket, prefix from yaml.\n",
    "    arg:\n",
    "        bucket: Name of the S3 bucket.\n",
    "        prefix: prefix for yaml file\n",
    "        catalog_name: name of catalog file\n",
    "    return:\n",
    "        'catalog' pandas object\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    obj = s3.get_object(Bucket=bucket, Key='{}/{}'.format(prefix, catalog_name))\n",
    "    catalog = pd.read_csv(obj['Body'], sep=\" \")\n",
    "    return catalog\n",
    "\n",
    "\n",
    "def ard_generation(sub_catalog, img_fullpth_catalog, bucket, aoi, tile_id, proj, bounds, n_row, n_col, tmp_pth, logger,\n",
    "                   dry_lower_ordinal, dry_upper_ordinal, wet_lower_ordinal, wet_upper_ordinal):\n",
    "    \"\"\"\n",
    "    generate ARD image per image in sub catalog.\n",
    "    arg:\n",
    "        sub_catalog: a list recording all images for the focused tile_ids\n",
    "        img_fullpth_catalog: a catalog for recording full uri path for each planet image\n",
    "        bucket: Name of the S3 bucket.\n",
    "        aoi: aoi to be processed\n",
    "        tile_id: id of current tile to be processed\n",
    "        proj: the projection of outputted ARD\n",
    "        bounds: xmin, xmax, ymin, ymax defining the extent of ard\n",
    "        tmp_path: tmp path defining the path for storing temporal files\n",
    "        logger: logging object\n",
    "        dry_lower_ordinal: lower bounds of ordinal days for dry season\n",
    "        dry_upper_ordinal: upper bounds of ordinal days for dry season\n",
    "        wet_lower_ordinal: lower bounds of ordinal days for wet season\n",
    "        wet_upper_ordinal: upper bounds of ordinal days for wet season\n",
    "    return:\n",
    "        'extent_geojson' sharply geojson object\n",
    "    \"\"\"\n",
    "    # initialize a record list for clear observations for each days\n",
    "    clear_records = [0] * 366\n",
    "    imgname_records = [0] * 366\n",
    "\n",
    "    # local_tile_folder: tmp folder for saving ard in the instance\n",
    "    local_tile_folder = os.path.join(tmp_pth, 'aoi{}_tile{}'.format(aoi, tile_id))\n",
    "    if not os.path.exists(local_tile_folder):\n",
    "        os.mkdir(local_tile_folder)\n",
    "\n",
    "    # convert entire data frame into a series of values\n",
    "    s = img_fullpth_catalog.stack()\n",
    "\n",
    "    # iterate over each planet image for focused tile_id\n",
    "    for i in range(len(sub_catalog)):\n",
    "        img_name = sub_catalog.iloc[i,0]\n",
    "        # sub_img_name = get_matching_s3_keys(bucket, prefix=prefix_x, suffix=\"{}_3B_AnalyticMS_SR.tif\".format(img_name))\n",
    "        single_img_pth = img_fullpth_catalog.iloc[s[s.str.contains(img_name,na=False)].index.get_level_values(0)].values[0][0]\n",
    "        if single_img_pth is None:\n",
    "            continue\n",
    "        single_msk_pth = single_img_pth.replace('AnalyticMS_SR', 'AnalyticMS_DN_udm')\n",
    "\n",
    "        # note that gdal and rasterio uri formats are different\n",
    "        uri_img_gdal = \"/vsis3/{}/{}\".format(bucket, single_img_pth)\n",
    "        uri_msk_gdal = \"/vsis3/{}/{}\".format(bucket, single_msk_pth)\n",
    "\n",
    "        ordinal_dates = datetime.strptime(img_name[0:8], '%Y%m%d').date().toordinal()\n",
    "        if ordinal_dates not in range(dry_lower_ordinal, dry_upper_ordinal + 1) and ordinal_dates \\\n",
    "                not in range(wet_lower_ordinal, wet_upper_ordinal + 1):\n",
    "            continue\n",
    "\n",
    "        doy = datetime.strptime(img_name[0:8], '%Y%m%d').date().timetuple().tm_yday\n",
    "\n",
    "        outname = \"PLANET%s%s\" % (str(datetime.strptime(img_name[0:8], '%Y%m%d').date().year),\n",
    "                                  str(\"{0:0=3d}\".format(doy)))\n",
    "\n",
    "        img = gdal.Open(uri_img_gdal)\n",
    "        msk = gdal.Open(uri_msk_gdal)\n",
    "\n",
    "        # img or msk is missing, give a warning and then skip\n",
    "        if img is None:\n",
    "            logger.warn(\"couldn't find {} from s3\".format(uri_img_gdal))\n",
    "            continue\n",
    "\n",
    "        if msk is None:\n",
    "            logger.warn(\"couldn't find {} from s3\".format(uri_msk_gdal))\n",
    "            continue\n",
    "\n",
    "        out_img = gdal.Warp(os.path.join(local_tile_folder, '_tmp_img'), img, outputBounds=[bounds[0], bounds[2], bounds[1],\n",
    "                                                                                  bounds[3]],\n",
    "                            width=n_col, height=n_row, dstNodata=-9999, outputType=gdal.GDT_Int16, dstSRS=proj)\n",
    "        out_msk = gdal.Warp(os.path.join(local_tile_folder, '_tmp_msk'), msk, outputBounds=[bounds[0], bounds[2], bounds[1],\n",
    "                                                                                  bounds[3]], width=n_col, height=n_row,\n",
    "                            dstNodata=-9999, outputType=gdal.GDT_Int16, dstSRS=proj)\n",
    "\n",
    "        n_valid_pixels = len(out_img.GetRasterBand(1).ReadAsArray()[out_img.GetRasterBand(1).ReadAsArray() > -9999])\n",
    "        n_clear_pixels = len(out_msk.GetRasterBand(1).ReadAsArray()[out_msk.GetRasterBand(1).ReadAsArray() == 0])\n",
    "\n",
    "        # firstly, see if clear observation is more than the record; if not, not necessary to process\n",
    "        if n_clear_pixels < clear_records[doy-1]:\n",
    "            continue\n",
    "        else:\n",
    "            if n_clear_pixels/n_valid_pixels > 0.2:\n",
    "\n",
    "                # if already created, delete old files\n",
    "                if clear_records[doy-1] > 0:\n",
    "                    os.remove(os.path.join(local_tile_folder, imgname_records[doy-1]))\n",
    "                    os.remove(os.path.join(local_tile_folder, imgname_records[doy-1]+'.hdr'))\n",
    "\n",
    "                out_img_b1_med = ndimage.median_filter(out_img.GetRasterBand(1).ReadAsArray(), size=3)\n",
    "                out_img_b2_med = ndimage.median_filter(out_img.GetRasterBand(2).ReadAsArray(), size=3)\n",
    "                out_img_b3_med = ndimage.median_filter(out_img.GetRasterBand(3).ReadAsArray(), size=3)\n",
    "                out_img_b4_med = ndimage.median_filter(out_img.GetRasterBand(4).ReadAsArray(), size=3)\n",
    "\n",
    "                clear_records[doy-1] = n_clear_pixels\n",
    "                imgname_records[doy-1] = outname + img_name[8:len(img_name)]\n",
    "                outdriver1 = gdal.GetDriverByName(\"ENVI\")\n",
    "                outdata = outdriver1.Create(os.path.join(local_tile_folder,\n",
    "                                                         outname+img_name[8:len(img_name)]),\n",
    "                                            n_col, n_row, 5, gdal.GDT_Int16, options=[\"INTERLEAVE=BIP\"])\n",
    "                outdata.GetRasterBand(1).WriteArray(out_img_b1_med)\n",
    "                outdata.FlushCache()\n",
    "                outdata.GetRasterBand(2).WriteArray(out_img_b2_med)\n",
    "                outdata.FlushCache()\n",
    "                outdata.GetRasterBand(3).WriteArray(out_img_b3_med)\n",
    "                outdata.FlushCache()\n",
    "                outdata.GetRasterBand(4).WriteArray(out_img_b4_med)\n",
    "                outdata.FlushCache()\n",
    "                outdata.GetRasterBand(5).WriteArray(out_msk.GetRasterBand(1).ReadAsArray())\n",
    "                outdata.FlushCache()\n",
    "\n",
    "                outdata.SetGeoTransform(out_img.GetGeoTransform())\n",
    "                outdata.FlushCache()\n",
    "                outdata.SetProjection(proj)\n",
    "                outdata.FlushCache()\n",
    "\n",
    "                del outdata\n",
    "\n",
    "        img = None\n",
    "        msk = None\n",
    "\n",
    "    # delete tmp image and mask\n",
    "    try:\n",
    "        os.remove(os.path.join(local_tile_folder, '_tmp_img'))\n",
    "    except OSError as e:\n",
    "        logger.info(\"Error for removing composite image {}: {} \".format(os.path.join(local_tile_folder, '_tmp_img'), e.strerror))\n",
    "\n",
    "    try:\n",
    "        os.remove(os.path.join(local_tile_folder, '_tmp_msk'))\n",
    "    except OSError as e:\n",
    "        logger.info(\"Error for removing composite image {}: {} \".format(os.path.join(local_tile_folder, 'tmp_msk'), e.strerror))\n",
    "\n",
    "\n",
    "def composite_generation(compositing_exe_path, bucket, prefix, foc_gpd_tile, tile_id, ard_folder, tmp_pth,\n",
    "                         logger, dry_lower_ordinal, dry_upper_ordinal, wet_lower_ordinal, wet_upper_ordinal, bsave_ard):\n",
    "    \"\"\"\n",
    "    generate composite image in sub catalog.\n",
    "    arg:\n",
    "        compositing_exe_path: directory for compositing exe\n",
    "        bucket: Name of the S3 bucket.\n",
    "        prefix: the prefix for tile_folder and img_folder\n",
    "        foc_gpd_tile: geopandas object indicating the extent of a focused tile, using GCS system\n",
    "        tile_id: id of current tile to be processed\n",
    "        ard_folder: the folder name for storing ard images\n",
    "        tmp_pth: the outputted folder for pcs and gcs images\n",
    "        logger: logging object\n",
    "        dry_lower_ordinal: lower bounds of ordinal days for dry season\n",
    "        dry_upper_ordinal: upper bounds of ordinal days for dry season\n",
    "        wet_lower_ordinal: lower bounds of ordinal days for wet season\n",
    "        wet_upper_ordinal: upper bounds of ordinal days for wet season\n",
    "        bsave_ard: if save ard images\n",
    "    \"\"\"\n",
    "\n",
    "    # fetch tile info, which will be used to crop intermediate compositing image\n",
    "    extent_geojson_gcs = mapping(foc_gpd_tile['geometry'])\n",
    "    txmin = extent_geojson_gcs['bbox'][0]\n",
    "    txmax = extent_geojson_gcs['bbox'][2]\n",
    "    tymin = extent_geojson_gcs['bbox'][1]\n",
    "    tymax = extent_geojson_gcs['bbox'][3]\n",
    "\n",
    "    ###########################################\n",
    "    #         compositing dryseason          #\n",
    "    ###########################################\n",
    "    cmd = [compositing_exe_path, ard_folder, tmp_pth, str(tile_id), str(dry_lower_ordinal), str(dry_upper_ordinal)]\n",
    "    # run composite exe\n",
    "    try:\n",
    "        p = subprocess.check_output(cmd, stderr=subprocess.STDOUT)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.error(\"compositing error for tile {} at dry season: {}\".format(tile_id, e))\n",
    "\n",
    "    #######################################################\n",
    "    #         reproject and crop compositing              #\n",
    "    #         image to align with GCS tile system         #\n",
    "    #######################################################\n",
    "    out_path_pcs_dry = os.path.join(tmp_pth, 'tile{}_{}_{}_pcs.tif'.format(tile_id, dry_lower_ordinal, dry_upper_ordinal))\n",
    "    out_path_gcs_dry = os.path.join(tmp_pth, 'tile{}_{}_{}.tif'.format(tile_id, dry_lower_ordinal, dry_upper_ordinal))\n",
    "    img = gdal.Open(out_path_pcs_dry)\n",
    "    if img is None:\n",
    "        logger.error(\"couldn't find pcs-based compositing result for tile {}\".format(tile_id))\n",
    "        return\n",
    "\n",
    "    out_img = gdal.Warp(out_path_gcs_dry, img, outputBounds=[txmin, tymin, txmax, tymax], resampleAlg=gdal.GRA_Bilinear, width=2000,\n",
    "                        height=2000, dstNodata=-9999, outputType=gdal.GDT_Int16, dstSRS='EPSG:4326')\n",
    "\n",
    "    # release memory\n",
    "    out_img = None\n",
    "    img = None\n",
    "\n",
    "    ##############################################\n",
    "    #            compositing wet season          #\n",
    "    ##############################################\n",
    "    cmd = [compositing_exe_path, ard_folder,tmp_pth, str(tile_id), str(wet_lower_ordinal), str(wet_upper_ordinal)]\n",
    "    # run composite exe\n",
    "    try:\n",
    "        p = subprocess.check_output(cmd, stderr=subprocess.STDOUT)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.error(\"compositing error for tile {} at wet season: {}\".format(tile_id, e))\n",
    "\n",
    "    #######################################################\n",
    "    #         reproject and crop compositing              #\n",
    "    #         image to align with GCS tile system         #\n",
    "    #######################################################\n",
    "    out_path_pcs_wet = os.path.join(tmp_pth, 'tile{}_{}_{}_pcs.tif'.format(tile_id, wet_lower_ordinal, wet_upper_ordinal))\n",
    "    out_path_gcs_wet = os.path.join(tmp_pth, 'tile{}_{}_{}.tif'.format(tile_id, wet_lower_ordinal, wet_upper_ordinal))\n",
    "    img = gdal.Open(out_path_pcs_wet)\n",
    "    if img is None:\n",
    "        logger.error(\"couldn't find pcs-based compositing result for tile {}\".format(tile_id))\n",
    "        return\n",
    "\n",
    "    out_img = gdal.Warp(out_path_gcs_wet, img, outputBounds=[txmin, tymin, txmax, tymax], resampleAlg=gdal.GRA_Bilinear, width=2000,\n",
    "                        height=2000, dstNodata=-9999, outputType=gdal.GDT_Int16, dstSRS='EPSG:4326')\n",
    "\n",
    "    out_img = None\n",
    "    img = None\n",
    "\n",
    "\n",
    "    #######################################################\n",
    "    #          upload compositing image to s3             #\n",
    "    #######################################################\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.upload_file(out_path_gcs_dry, bucket, '{}/composite_sr/OS/tile{}_{}_{}.tif'.format(prefix, tile_id, dry_lower_ordinal,\n",
    "                                                                                          dry_upper_ordinal))\n",
    "    s3.upload_file(out_path_gcs_wet, bucket, '{}/composite_sr/GS/tile{}_{}_{}.tif'.format(prefix, tile_id, wet_lower_ordinal,\n",
    "                                                                                          wet_upper_ordinal))\n",
    "\n",
    "    #######################################################\n",
    "    #          delete local composte files                #\n",
    "    #######################################################\n",
    "    try:\n",
    "        os.remove(os.path.join(tmp_pth, out_path_gcs_dry))\n",
    "    except OSError as e:\n",
    "        logger.info(\"Error for removing composite image {}: {} \".format(out_path_gcs_dry, e.strerror))\n",
    "\n",
    "    try:\n",
    "        os.remove(os.path.join(tmp_pth, out_path_pcs_dry))\n",
    "    except OSError as e:\n",
    "        logger.info(\"Error for removing composite image {}: {} \".format(out_path_pcs_dry, e.strerror))\n",
    "\n",
    "    try:\n",
    "        os.remove(os.path.join(tmp_pth, out_path_gcs_wet))\n",
    "    except OSError as e:\n",
    "        logger.info(\"Error for removing composite image {}: {} \".format(out_path_gcs_wet, e.strerror))\n",
    "\n",
    "    try:\n",
    "        os.remove(os.path.join(tmp_pth, out_path_pcs_wet))\n",
    "    except OSError as e:\n",
    "        logger.info(\"Error for removing composite image {}: {} \".format(out_path_pcs_wet, e.strerror))\n",
    "\n",
    "    if bsave_ard is False:\n",
    "        # # Try to delete the ard image folder\n",
    "        try:\n",
    "            shutil.rmtree(ard_folder)\n",
    "        except OSError as e:  # if failed, report it back to the user ##\n",
    "            logger.info(\"Error for removing ARD folder {}: {} \".format(ard_folder, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/composite/lib/python3.7/site-packages/ipykernel_launcher.py:129: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "config_filename = 'cvmapper_config_composite.yaml'\n",
    "tile_id = 513679\n",
    "bsave_ard = False\n",
    "s3_bucket = 'activemapper'\n",
    "\n",
    "\n",
    "# define res\n",
    "res = 3\n",
    "\n",
    "# define compositing ext\n",
    "compositing_exe_path = '/home/ubuntu/imager/C/AFMapTSComposite/bin/composite'\n",
    "\n",
    "# define log path\n",
    "log_path = '%s/log/planet_composite.log' % os.environ['HOME']\n",
    "logging.basicConfig(filename=log_path, filemode='w', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "tmp_pth = '/tmp'\n",
    "\n",
    "# parse mapper parameter from yaml\n",
    "params = parse_yaml_from_s3(s3_bucket, config_filename)['mapper']\n",
    "\n",
    "# read individual parameters\n",
    "prefix = params['prefix']\n",
    "\n",
    "# fetching a table linking  planet images name and tile id\n",
    "img_catalog_name = params['img_catalog_name']\n",
    "\n",
    "# fetching a table recording full pth of planet images\n",
    "img_catalog_pth = params['img_catalog_pth']\n",
    "\n",
    "# a geojson indicating the extent of each tile\n",
    "tiles_geojson_path = params['tile_geojson_path']\n",
    "\n",
    "\n",
    "aoi = params['aoi']\n",
    "\n",
    "# define lower and upper bounds for dry and wet season\n",
    "dry_lower_ordinal = params['dry_lower_ordinal'] # 2017/12/01\n",
    "dry_upper_ordinal = params['dry_upper_ordinal'] # 2018/02/28\n",
    "wet_lower_ordinal = params['wet_lower_ordinal'] # 2018/05/01\n",
    "wet_upper_ordinal = params['wet_upper_ordinal'] # 2018/09/30\n",
    "\n",
    "# time zone\n",
    "tz = timezone('US/Eastern')\n",
    "logger.info(\"Progress: starting making ARD images for aoi_{} ({})\".format(aoi, datetime.now(tz).strftime('%Y-%m-%d %H:%M:%S')))\n",
    "\n",
    "# read a catalog for linking planet images and tile id\n",
    "img_catalog = parse_catalog_from_s3(s3_bucket, prefix, img_catalog_name)\n",
    "\n",
    "# read a catalog recording full path for planet images\n",
    "img_fullpth_catalog = parse_catalog_from_s3(s3_bucket, prefix, img_catalog_pth)\n",
    "\n",
    "# read a geopandas object for tile geojson\n",
    "uri_tile = \"s3://{}/{}/{}\".format(s3_bucket, prefix, tiles_geojson_path)\n",
    "gpd_tile = gpd.read_file(uri_tile)\n",
    "if gpd_tile is None:\n",
    "    logger.error(\"reading {} failed\". format(uri_tile))\n",
    "\n",
    "foc_img_catalog = img_catalog.loc[img_catalog['tile'] == int(tile_id)]\n",
    "sample_img_nm = foc_img_catalog.iloc[0, 0]\n",
    "foc_gpd_tile = gpd_tile[gpd_tile['tile'] == int(tile_id)]\n",
    "tile_geojson, proj = get_geojson_pcs(s3_bucket, foc_gpd_tile, sample_img_nm, img_fullpth_catalog, logger)\n",
    "bounds, (n_row, n_col) = get_extent(tile_geojson, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'foctiles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a258db74e657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m logger.info(\"Progress: finished ARD generation for tile_id {}, and the total finished tiles is {} ({}))\".format(i + 1, foctiles.iloc[i],\n\u001b[0m\u001b[1;32m      6\u001b[0m                                                                                      \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                                                      strftime('%Y-%m-%d %H:%M:%S'),))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'foctiles' is not defined"
     ]
    }
   ],
   "source": [
    "ard_generation(foc_img_catalog, img_fullpth_catalog, s3_bucket,  aoi, int(tile_id),  proj, bounds, n_row, n_col, tmp_pth,\n",
    "               logger, dry_lower_ordinal, dry_upper_ordinal, wet_lower_ordinal, wet_upper_ordinal)\n",
    "\n",
    "\n",
    "logger.info(\"Progress: finished ARD generation for tile_id {}, and the total finished tiles is {} ({}))\".format(tile_id, i + 1,\n",
    "                                                                                     datetime.now(tz).\n",
    "                                                                                     strftime('%Y-%m-%d %H:%M:%S'),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for removing //tmp/aoi1479_tile513679 .\n"
     ]
    }
   ],
   "source": [
    "composite_generation(compositing_exe_path, s3_bucket, prefix,  foc_gpd_tile, tile_id,\n",
    "                     os.path.join(tmp_pth, 'aoi{}_tile{}'.format(aoi, tile_id)), tmp_pth, logger, dry_lower_ordinal,\n",
    "                     dry_upper_ordinal, wet_lower_ordinal, wet_upper_ordinal, bsave_ard)\n",
    "\n",
    "logger.info(\"Progress: finished compositing for tile_id {}, and the total finished tiles is {} ({}))\".format(tile_id, i + 1, \n",
    "                                                                                             datetime.now(tz).\n",
    "                                                                                             strftime('%Y-%m-%d %H:%M:%S'),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e6cde6e6c584>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# ARD generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m ard_generation(img_catalog, img_fullpth_catalog, s3_bucket,  aoi, int(tile_id),  proj, bounds, nrow, ncol, tmp_pth,\n\u001b[0;32m----> 9\u001b[0;31m                logger, dry_lower_ordinal, dry_upper_ordinal, wet_lower_ordinal, wet_upper_ordinal)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9686c22d9f79>\u001b[0m in \u001b[0;36mard_generation\u001b[0;34m(sub_catalog, img_fullpth_catalog, bucket, aoi, tile_id, proj, bounds, n_row, n_col, tmp_pth, logger, dry_lower_ordinal, dry_upper_ordinal, wet_lower_ordinal, wet_upper_ordinal)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mn_clear_pixels\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_valid_pixels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# if already created, delete old files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# ARD generation\n",
    "ard_generation(img_catalog, img_fullpth_catalog, s3_bucket,  aoi, int(tile_id),  proj, bounds, nrow, ncol, tmp_pth,\n",
    "               logger, dry_lower_ordinal, dry_upper_ordinal, wet_lower_ordinal, wet_upper_ordinal)\n",
    "\n",
    "\n",
    "logger.info(\"Progress: finished ARD generation for tile_id {}, and the total finished tiles is {} ({}))\".format(i + 1, foctiles.iloc[i],\n",
    "                                                                                     datetime.now(tz).\n",
    "                                                                                     strftime('%Y-%m-%d %H:%M:%S'),))\n",
    "\n",
    "# compositing\n",
    "composite_generation(compositing_exe_path, s3_bucket, prefix,  foc_gpd_tile, tile_id,\n",
    "                     os.path.join(tmp_pth, 'aoi{}_tile{}'.format(aoi, tile_id)), tmp_pth, logger, dry_lower_ordinal,\n",
    "                     dry_upper_ordinal, wet_lower_ordinal, wet_upper_ordinal, bsave_ard)\n",
    "\n",
    "logger.info(\"Progress: finished compositing for tile_id {}, and the total finished tiles is {} ({}))\".format(i + 1, foctiles.iloc[i],\n",
    "                                                                                             datetime.now(tz).\n",
    "                                                                                             strftime('%Y-%m-%d %H:%M:%S'),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "if bsave_ard is False:\n",
    "    # # Try to delete the ard image folder\n",
    "    try:\n",
    "        shutil.rmtree(ard_folder)\n",
    "    except OSError as e:  # if failed, report it back to the user ##\n",
    "        logger.info(\"Error for removing ARD folder {}: {} \".format(ard_folder, e.strerror))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:composite]",
   "language": "python",
   "name": "conda-env-composite-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
