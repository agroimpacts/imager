{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module is developed to automated the process of making composite images in parallel for MappingAfrica project. The whole process\n",
    "is consisted of two steps: 1) make planet ARD images and 2) call AFMapTSComposite (c-based exe) for making composites\n",
    "The module can be called by using one of three modes: 1) tile-based, 2)csv-based and 3) aoi based\n",
    "Tile and csv-based mode are mainly for testing usage; aoi-based mode is for on-production\n",
    "Author: Su Ye (sye@clarku.edu)\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import gdal\n",
    "import geopandas as gpd\n",
    "import osr\n",
    "from shapely.geometry import mapping\n",
    "from math import ceil\n",
    "from datetime import datetime\n",
    "import os\n",
    "import click\n",
    "from scipy import ndimage\n",
    "import logging\n",
    "import time\n",
    "import yaml\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "from pytz import timezone\n",
    "from fixed_thread_pool_executor import FixedThreadPoolExecutor\n",
    "from osgeo import gdal_array\n",
    "\n",
    "import shutil\n",
    "\n",
    "# (this function has been abandoned in the current version,  cause the searching efficiency over s3 is low)\n",
    "def get_matching_s3_keys(bucket, prefix='', suffix=''):\n",
    "    \"\"\"\n",
    "    Generate the keys in an S3 bucket.\n",
    "    arg:\n",
    "        bucket: Name of the S3 bucket.\n",
    "        prefix: Only fetch keys that start with this prefix (optional).\n",
    "        suffix: Only fetch keys that end with this suffix (optional).\n",
    "    return:\n",
    "        (string) key\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    kwargs = {'Bucket': bucket}\n",
    "\n",
    "    # If the prefix is a single string (not a tuple of strings), we can\n",
    "    # do the filtering directly in the S3 API.\n",
    "    if isinstance(prefix, str):\n",
    "        kwargs['Prefix'] = prefix\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # The S3 API response is a large blob of metadata.\n",
    "        # 'Contents' contains information about the listed objects.\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "        for obj in resp['Contents']:\n",
    "            key = obj['Key']\n",
    "            if key.startswith(prefix) and key.endswith(suffix):\n",
    "                return key\n",
    "\n",
    "        # The S3 API is paginated, returning up to 1000 keys at a time.\n",
    "        # Pass the continuation token into the next response, until we\n",
    "        # reach the final page (when this field is missing).\n",
    "        try:\n",
    "            kwargs['ContinuationToken'] = resp['NextContinuationToken']\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "\n",
    "def get_geojson_pcs(bucket, gpd_tile, sample_img_nm, img_fullpth_catalog, logger):\n",
    "    \"\"\"\n",
    "    covert geojson of a tile to pcs of sample image.\n",
    "    arg:\n",
    "        bucket: Name of the S3 bucket.\n",
    "        gpd_tile: geopandas object\n",
    "        tile_folder: the folder name for storing tile geojson\n",
    "        sample_img_nm: the name of sample image used to extract pcs\n",
    "        logger: logger\n",
    "    return:\n",
    "        'extent_geojson' sharply geojson object\n",
    "        'proj' projection object\n",
    "    \"\"\"\n",
    "\n",
    "    # read projection from sample planet image\n",
    "    #sub_img_pth = get_matching_s3_keys(bucket, prefix=prefix_x, suffix=\"{}_3B_AnalyticMS_SR.tif\".format(sample_img_nm))\n",
    "    s = img_fullpth_catalog.stack()  # convert entire data frame into a series of values\n",
    "    sub_img_pth = img_fullpth_catalog.iloc[s[s.str.contains(sample_img_nm,na=False)].index.get_level_values(0)].values[0][0]\n",
    "    uri_img_gdal = \"/vsis3/{}/{}\".format(bucket, sub_img_pth)\n",
    "    img = gdal.Open(uri_img_gdal)\n",
    "    if img is None:\n",
    "        logger.error(\"reading {} failed\". format(uri_img_gdal))\n",
    "\n",
    "    # convert tile to planet image pcs\n",
    "    gpd_tile_pcs = gpd_tile.to_crs(epsg=osr.SpatialReference(wkt=img.GetProjection()).GetAttrValue('AUTHORITY',1))\n",
    "    extent_geojson = mapping(gpd_tile_pcs['geometry'])\n",
    "    proj = img.GetProjectionRef()\n",
    "    img = None\n",
    "    return extent_geojson, proj\n",
    "\n",
    "\n",
    "def get_extent(extent_geojson, res):\n",
    "    \"\"\"\n",
    "    read geojson of a tile from an S3 bucket, and convert projection to be aligned with sample image.\n",
    "    arg:\n",
    "        'extent_geojson': sharply geojson object\n",
    "        res: planet resolution\n",
    "    return:\n",
    "        (float, float, float, float), (int, int)) tuple\n",
    "    \"\"\"\n",
    "    # txmin = min([row[0] for row in extent_geojson['coordinates'][0]]) - res / 2.0\n",
    "    # txmax = max([row[0] for row in extent_geojson['coordinates'][0]]) + res / 2.0\n",
    "    # tymin = min([row[1] for row in extent_geojson['coordinates'][0]]) - res / 2.0\n",
    "    # tymax = max([row[1] for row in extent_geojson['coordinates'][0]]) + res / 2.0\n",
    "    txmin = extent_geojson['bbox'][0] - res * 20\n",
    "    txmax = extent_geojson['bbox'][2] + res * 20\n",
    "    tymin = extent_geojson['bbox'][1] - res * 20\n",
    "    tymax = extent_geojson['bbox'][3] + res * 20\n",
    "    n_row = ceil((tymax - tymin)/res)\n",
    "    n_col = ceil((txmax - txmin)/res)\n",
    "    txmin_new = (txmin + txmax)/2 - n_row / 2 * res\n",
    "    txmax_new = (txmin + txmax)/2 + n_row / 2 * res\n",
    "    tymin_new = (tymin + tymax)/2 - n_col / 2 * res\n",
    "    tymax_new = (tymin + tymax)/2 + n_col / 2 * res\n",
    "    return (txmin_new, txmax_new, tymin_new, tymax_new), (n_row, n_col)\n",
    "\n",
    "\n",
    "def parse_yaml_from_s3(bucket, prefix):\n",
    "    \"\"\"\n",
    "    read bucket, prefix from yaml.\n",
    "    arg:\n",
    "        bucket: Name of the S3 bucket.\n",
    "        prefix: the name for yaml file\n",
    "    return:\n",
    "        yaml object\n",
    "    \"\"\"\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Bucket(bucket).Object(prefix).get()['Body'].read()\n",
    "    return yaml.load(obj)\n",
    "\n",
    "\n",
    "def parse_catalog_from_s3(bucket, prefix, catalog_name):\n",
    "    \"\"\"\n",
    "    read bucket, prefix from yaml.\n",
    "    arg:\n",
    "        bucket: Name of the S3 bucket.\n",
    "        prefix: prefix for yaml file\n",
    "        catalog_name: name of catalog file\n",
    "    return:\n",
    "        'catalog' pandas object\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    obj = s3.get_object(Bucket=bucket, Key='{}/{}'.format(prefix, catalog_name))\n",
    "    catalog = pd.read_csv(obj['Body'], sep=\" \")\n",
    "    return catalog\n",
    "\n",
    "\n",
    "def delete_file(file_pth, logger):\n",
    "    \"\"\"\n",
    "    delete the file given a specific path\n",
    "    arg:\n",
    "        file_pth: full path of file to delete.\n",
    "        logger: handler of logging file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.remove(file_pth)\n",
    "    except OSError as e:\n",
    "        logger.warning(\"Removing {} fails: {} \".format(file_pth, e.strerror))\n",
    "\n",
    "\n",
    "def run_cmd(cmd, logger):\n",
    "    \"\"\"\n",
    "    using os to run a command line\n",
    "    arg:\n",
    "        cmd: a command line\n",
    "        logger: handler of logging file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.system(cmd)\n",
    "    except OSError as e:\n",
    "        logger.error(\"Runing command line '{}' fails: {}\".format(cmd, e))\n",
    "        raise\n",
    "\n",
    "\n",
    "def is_valid_image(path):\n",
    "    ds = gdal.Open(path)\n",
    "    if ds is None:\n",
    "        return False\n",
    "\n",
    "    rasterArray = np.array(ds.GetRasterBand(1).ReadAsArray())\n",
    "    unique_val = np.unique(rasterArray)\n",
    "    if len(unique_val) == 1:\n",
    "        del ds\n",
    "        return False\n",
    "    else:\n",
    "        del ds\n",
    "        return True\n",
    "\n",
    "\n",
    "def ard_generation(sub_catalog, img_fullpth_catalog, bucket, tile_id, proj, bounds, n_row, n_col, tmp_pth, logger,\n",
    "                   dry_lower_ordinal, dry_upper_ordinal, wet_lower_ordinal, wet_upper_ordinal):\n",
    "    \"\"\"\n",
    "    generate ARD image per image in sub catalog.\n",
    "    arg:\n",
    "        sub_catalog: a list recording all images for the focused tile_ids\n",
    "        img_fullpth_catalog: a catalog for recording full uri path for each planet image\n",
    "        bucket: Name of the S3 bucket.\n",
    "        tile_id: id of current tile to be processed\n",
    "        proj: the projection of outputted ARD\n",
    "        bounds: xmin, xmax, ymin, ymax defining the extent of ard\n",
    "        tmp_path: tmp path defining the path for storing temporal files\n",
    "        logger: logging object\n",
    "        dry_lower_ordinal: lower bounds of ordinal days for dry season\n",
    "        dry_upper_ordinal: upper bounds of ordinal days for dry season\n",
    "        wet_lower_ordinal: lower bounds of ordinal days for wet season\n",
    "        wet_upper_ordinal: upper bounds of ordinal days for wet season\n",
    "    return:\n",
    "        'extent_geojson' sharply geojson object\n",
    "    \"\"\"\n",
    "    # initialize a record list for clear observations for each days\n",
    "    clear_records = [0] * 366\n",
    "    imgname_records = [0] * 366\n",
    "\n",
    "    # local_tile_folder: tmp folder for saving ard in the instance\n",
    "    local_tile_folder = os.path.join(tmp_pth, 'tile{}'.format(tile_id))\n",
    "    if not os.path.exists(local_tile_folder):\n",
    "        os.mkdir(local_tile_folder)\n",
    "\n",
    "    # convert entire data frame into a series of values\n",
    "    s = img_fullpth_catalog.stack()\n",
    "\n",
    "    # iterate over each planet image for focused tile_id\n",
    "    for i in range(len(sub_catalog)):\n",
    "        img_name = sub_catalog.iloc[i,0]\n",
    "        # sub_img_name = get_matching_s3_keys(bucket, prefix=prefix_x, suffix=\"{}_3B_AnalyticMS_SR.tif\".format(img_name))\n",
    "        single_img_pth = img_fullpth_catalog.iloc[s[s.str.contains(img_name,na=False)].index.get_level_values(0)].values[0][0]\n",
    "        if single_img_pth is None:\n",
    "            continue\n",
    "        single_msk_pth = single_img_pth.replace('AnalyticMS_SR', 'AnalyticMS_DN_udm')\n",
    "\n",
    "        # note that gdal and rasterio uri formats are different\n",
    "        uri_img_gdal = \"/vsis3/{}/{}\".format(bucket, single_img_pth)\n",
    "        uri_msk_gdal = \"/vsis3/{}/{}\".format(bucket, single_msk_pth)\n",
    "\n",
    "        ordinal_dates = datetime.strptime(img_name[0:8], '%Y%m%d').date().toordinal()\n",
    "        if ordinal_dates not in range(dry_lower_ordinal, dry_upper_ordinal + 1) and ordinal_dates \\\n",
    "                not in range(wet_lower_ordinal, wet_upper_ordinal + 1):\n",
    "            continue\n",
    "\n",
    "        doy = datetime.strptime(img_name[0:8], '%Y%m%d').date().timetuple().tm_yday\n",
    "\n",
    "        outname = \"PLANET%s%s\" % (str(datetime.strptime(img_name[0:8], '%Y%m%d').date().year),\n",
    "                                  str(\"{0:0=3d}\".format(doy)))\n",
    "\n",
    "        img = gdal.Open(uri_img_gdal)\n",
    "        msk = gdal.Open(uri_msk_gdal)\n",
    "\n",
    "        # img or msk is missing, give a warning and then skip\n",
    "        if img is None:\n",
    "            logger.warning(\"couldn't find {} from s3 for tile {}\".format(uri_img_gdal, tile_id))\n",
    "            continue\n",
    "\n",
    "        if msk is None:\n",
    "            logger.warning(\"couldn't find {} from s3 for tile {}\".format(uri_msk_gdal, tile_id))\n",
    "            continue\n",
    "\n",
    "        out_img = gdal.Warp(os.path.join(local_tile_folder, '_tmp_img'), img, outputBounds=[bounds[0], bounds[2], bounds[1],\n",
    "                                                                                  bounds[3]],\n",
    "                            width=n_col, height=n_row, dstNodata=-9999, outputType=gdal.GDT_Int16, dstSRS=proj)\n",
    "        out_msk = gdal.Warp(os.path.join(local_tile_folder, '_tmp_msk'), msk, outputBounds=[bounds[0], bounds[2], bounds[1],\n",
    "                                                                                  bounds[3]], width=n_col, height=n_row,\n",
    "                            dstNodata=-9999, outputType=gdal.GDT_Int16, dstSRS=proj)\n",
    "\n",
    "        if out_img is None:\n",
    "            logger.warning(\"Running gdal.Warp fails for {} for tile {}\".format(uri_img_gdal, tile_id))\n",
    "            continue\n",
    "\n",
    "        if out_msk is None:\n",
    "            logger.warning(\"Running gdal.Warp fails for {} for tile {}\".format(uri_msk_gdal, tile_id))\n",
    "            continue\n",
    "\n",
    "        n_valid_pixels = len(out_img.GetRasterBand(1).ReadAsArray()[out_img.GetRasterBand(1).ReadAsArray() > -9999])\n",
    "        n_clear_pixels = len(out_msk.GetRasterBand(1).ReadAsArray()[out_msk.GetRasterBand(1).ReadAsArray() == 0])\n",
    "\n",
    "        # firstly, see if clear observation is more than the record; if not, not necessary to process\n",
    "        if n_clear_pixels < clear_records[doy-1]:\n",
    "            continue\n",
    "        else:\n",
    "            if n_clear_pixels/n_valid_pixels > 0.2:\n",
    "                out_img_b1_med = ndimage.median_filter(out_img.GetRasterBand(1).ReadAsArray(), size=3)\n",
    "                out_img_b2_med = ndimage.median_filter(out_img.GetRasterBand(2).ReadAsArray(), size=3)\n",
    "                out_img_b3_med = ndimage.median_filter(out_img.GetRasterBand(3).ReadAsArray(), size=3)\n",
    "                out_img_b4_med = ndimage.median_filter(out_img.GetRasterBand(4).ReadAsArray(), size=3)\n",
    "        \n",
    "            \n",
    "                clear_records[doy-1] = n_clear_pixels\n",
    "                imgname_records[doy-1] = outname + img_name[8:len(img_name)]\n",
    "                outdriver1 = gdal.GetDriverByName(\"ENVI\")\n",
    "                outdata = outdriver1.Create(os.path.join(local_tile_folder,\n",
    "                                                    outname+img_name[8:len(img_name)]),\n",
    "                                            n_col, n_row, 5, gdal.GDT_Int16, options=[\"INTERLEAVE=BIP\"])\n",
    "                outdata.GetRasterBand(1).WriteArray(out_img_b1_med)\n",
    "                outdata.FlushCache()\n",
    "                outdata.GetRasterBand(2).WriteArray(out_img_b2_med)\n",
    "                outdata.FlushCache()\n",
    "                outdata.GetRasterBand(3).WriteArray(out_img_b3_med)\n",
    "                outdata.FlushCache()\n",
    "                outdata.GetRasterBand(4).WriteArray(out_img_b4_med)\n",
    "                outdata.FlushCache()\n",
    "                outdata.GetRasterBand(5).WriteArray(out_msk.GetRasterBand(1).ReadAsArray())\n",
    "                outdata.FlushCache()\n",
    "\n",
    "                outdata.SetGeoTransform(out_img.GetGeoTransform())\n",
    "                outdata.FlushCache()\n",
    "                outdata.SetProjection(proj)\n",
    "                outdata.FlushCache()\n",
    "\n",
    "                del outdata\n",
    "\n",
    "        img = None\n",
    "        msk = None\n",
    "\n",
    "        out_img = None\n",
    "        out_msk = None\n",
    "\n",
    "    # delete tmp image and mask\n",
    "    delete_file(os.path.join(local_tile_folder, '_tmp_img'), logger)\n",
    "    delete_file(os.path.join(local_tile_folder, '_tmp_msk'), logger)\n",
    "\n",
    "\n",
    "def composite_generation(compositing_exe_path, bucket, prefix, foc_gpd_tile, tile_id, ard_folder, tmp_pth,\n",
    "                         logger, dry_lower_ordinal, dry_upper_ordinal, wet_lower_ordinal, wet_upper_ordinal, bsave_ard, output_prefix):\n",
    "    \"\"\"\n",
    "    generate composite image in sub catalog.\n",
    "    arg:\n",
    "        compositing_exe_path: directory for compositing exe\n",
    "        bucket: Name of the S3 bucket.\n",
    "        prefix: the prefix for tile_folder and img_folder\n",
    "        foc_gpd_tile: geopandas object indicating the extent of a focused tile, using GCS system\n",
    "        tile_id: id of current tile to be processed\n",
    "        ard_folder: the folder name for storing ard images\n",
    "        tmp_pth: the outputted folder for pcs and gcs images\n",
    "        logger: logging object\n",
    "        dry_lower_ordinal: lower bounds of ordinal days for dry season\n",
    "        dry_upper_ordinal: upper bounds of ordinal days for dry season\n",
    "        wet_lower_ordinal: lower bounds of ordinal days for wet season\n",
    "        wet_upper_ordinal: upper bounds of ordinal days for wet season\n",
    "        bsave_ard: if save ard images\n",
    "        output_prefix: prefix for composite in S3 \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # fetch tile info, which will be used to crop intermediate compositing image\n",
    "    extent_geojson_gcs = mapping(foc_gpd_tile['geometry'])\n",
    "    txmin = extent_geojson_gcs['bbox'][0]\n",
    "    txmax = extent_geojson_gcs['bbox'][2]\n",
    "    tymin = extent_geojson_gcs['bbox'][1]\n",
    "    tymax = extent_geojson_gcs['bbox'][3]\n",
    "\n",
    "    #######################################################\n",
    "    #           1. begin compositing dryseason            #\n",
    "    #######################################################\n",
    "    cmd = [compositing_exe_path, ard_folder, tmp_pth, str(tile_id), str(dry_lower_ordinal), str(dry_upper_ordinal)]\n",
    "    # run composite exe\n",
    "    try:\n",
    "        p = subprocess.check_output(cmd, stderr=subprocess.STDOUT)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.error(\"compositing error for tile {} at dry season: {}\".format(tile_id, e))\n",
    "        raise\n",
    "\n",
    "    # reproject and crop compositing image to align with GCS tile system\n",
    "    out_path_pcs_dry = os.path.join(tmp_pth, 'tile{}_{}_{}_pcs.tif'.format(tile_id, dry_lower_ordinal, dry_upper_ordinal))\n",
    "    out_path_gcs_dry = os.path.join(tmp_pth, 'tile{}_{}_{}_gcs.tif'.format(tile_id, dry_lower_ordinal, dry_upper_ordinal))\n",
    "    out_path_gcs_dry_TCI = os.path.join(tmp_pth, 'tile{}_{}_{}_gcs_TCI.tif'.format(tile_id, dry_lower_ordinal, dry_upper_ordinal))\n",
    "    out_path_dry = os.path.join(tmp_pth, 'tile{}_{}_{}.tif'.format(tile_id, dry_lower_ordinal, dry_upper_ordinal))\n",
    "    \n",
    "\n",
    "    # here call gdalwarp directly instead of gdal.warp, cause unexpected bug for gdal.warp\n",
    "    # out_img = gdal.Warp(out_path_gcs_dry, img, outputBounds=[txmin, tymin, txmax, tymax], resampleAlg=gdal.GRA_Bilinear, width=2000,\n",
    "                        # height=2000, dstNodata=-9999,outputType=gdal.GDT_Int16, dstSRS='EPSG:4326')\n",
    "    cmd = 'gdalwarp -q -overwrite -t_srs EPSG:4326 -te {} {} {} {} -r bilinear -ts 2000 2000 -srcnodata -9999 -dstnodata -9999 -ot ' \\\n",
    "          'Int16 {} {}'.format(txmin, tymin, txmax, tymax, out_path_pcs_dry, out_path_gcs_dry)\n",
    "\n",
    "    run_cmd(cmd, logger)\n",
    "    \n",
    "    # check if the composite image is empty\n",
    "    if not is_valid_image(out_path_gcs_dry):\n",
    "        logger.error(\"Composition fails for the first time for tile{}_{}_{}\".format(tile_id, dry_lower_ordinal, dry_upper_ordinal))\n",
    "        cmd = [compositing_exe_path, ard_folder, tmp_pth, str(tile_id), str(dry_lower_ordinal), str(dry_upper_ordinal)]\n",
    "        # run composite exe\n",
    "        p = subprocess.check_output(cmd, stderr=subprocess.STDOUT)\n",
    "\n",
    "        cmd = 'gdalwarp -q -overwrite -t_srs EPSG:4326 -te {} {} {} {} -r bilinear -ts 2000 2000 -srcnodata -9999 -dstnodata -9999 -ot ' \\\n",
    "              'Int16 {} {}'.format(txmin, tymin, txmax, tymax, out_path_pcs_dry, out_path_gcs_dry)\n",
    "        run_cmd(cmd, logger)\n",
    "\n",
    "        if not is_valid_image(out_path_gcs_dry):\n",
    "            logger.error(\"compositing error for tile {} at dry season for twice\".format(tile_id))\n",
    "    \n",
    "\n",
    "    #######################################################################################\n",
    "    #                 convert to Cloud-Optimized Geotiff                                  #\n",
    "    #        (source: https://trac.osgeo.org/gdal/wiki/CloudOptimizedGeoTIFF)             #\n",
    "    #   why create a memory driver filer, not directly created:                           #\n",
    "    #   the problem is that this will give an error in the COG format, because the        # \n",
    "    #   pyramids were created after the tiling.                                           #\n",
    "    #######################################################################################\n",
    "    cmd = 'gdal_translate -q {} {} -co COMPRESS=LZW -co TILED=YES'. format(out_path_gcs_dry, out_path_gcs_dry_TCI)\n",
    "    run_cmd(cmd, logger)\n",
    "\n",
    "    cmd = 'gdaladdo -q -r average {} 2 4'. format(out_path_gcs_dry_TCI)\n",
    "    run_cmd(cmd, logger)\n",
    "\n",
    "    cmd = 'gdal_translate -q {} {} -co COMPRESS=LZW -co COPY_SRC_OVERVIEWS=YES -co TILED=YES'.format(out_path_gcs_dry_TCI,\n",
    "                                                                                                     out_path_dry)\n",
    "    run_cmd(cmd, logger)\n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    #            2. begin compositing wet season            #\n",
    "    #########################################################\n",
    "    cmd = [compositing_exe_path, ard_folder,tmp_pth, str(tile_id), str(wet_lower_ordinal), str(wet_upper_ordinal)]\n",
    "    # run composite exe\n",
    "    try:\n",
    "        p = subprocess.check_output(cmd, stderr=subprocess.STDOUT)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.error(\"compositing error for tile {} at wet season: {}\".format(tile_id, e))\n",
    "        raise\n",
    "\n",
    "    # reproject and crop compositing image to align with GCS tile system         \n",
    "    out_path_pcs_wet = os.path.join(tmp_pth, 'tile{}_{}_{}_pcs.tif'.format(tile_id, wet_lower_ordinal, wet_upper_ordinal))\n",
    "    out_path_gcs_wet = os.path.join(tmp_pth, 'tile{}_{}_{}_gcs.tif'.format(tile_id, wet_lower_ordinal, wet_upper_ordinal))\n",
    "    out_path_gcs_wet_TCI = os.path.join(tmp_pth, 'tile{}_{}_{}_gcs_TCI.tif'.format(tile_id, wet_lower_ordinal, wet_upper_ordinal))\n",
    "    out_path_wet = os.path.join(tmp_pth, 'tile{}_{}_{}.tif'.format(tile_id, wet_lower_ordinal, wet_upper_ordinal))\n",
    "        \n",
    "\n",
    "    # img = gdal.Open(out_path_pcs_wet)\n",
    "    # if img is None:\n",
    "        # logger.error(\"couldn't find pcs-based compositing result for tile {}\".format(tile_id))\n",
    "        # return\n",
    "\n",
    "    # out_img = gdal.Warp(out_path_gcs_wet, img, outputBounds=[txmin, tymin, txmax, tymax], resampleAlg=gdal.GRA_Bilinear, width=2000,\n",
    "                        #height=2000, dstNodata=-9999, xRes=0.05/2000, yRes=0.05/2000, outputType=gdal.GDT_Int16, dstSRS='EPSG:4326')\n",
    "\n",
    "    cmd = 'gdalwarp -q -overwrite -t_srs EPSG:4326 -te {} {} {} {} -r bilinear -ts 2000 2000 -srcnodata -9999 -dstnodata -9999 -ot ' \\\n",
    "          'Int16 {} {}'.format(txmin, tymin, txmax, tymax, out_path_pcs_wet, out_path_gcs_wet)\n",
    "    run_cmd(cmd, logger)\n",
    "\n",
    "    # check if the composite image is empty\n",
    "    if not is_valid_image(out_path_gcs_wet):\n",
    "        logger.error(\"Composition fails for the first time for tile{}_{}_{}\".format(tile_id, wet_lower_ordinal, wet_upper_ordinal))\n",
    "        cmd = [compositing_exe_path, ard_folder, tmp_pth, str(tile_id), str(wet_lower_ordinal), str(wet_upper_ordinal)]\n",
    "        # run composite exe\n",
    "        p = subprocess.check_output(cmd, stderr=subprocess.STDOUT)\n",
    "\n",
    "        cmd = 'gdalwarp -q -overwrite -t_srs EPSG:4326 -te {} {} {} {} -r bilinear -ts 2000 2000 -srcnodata -9999 -dstnodata -9999 -ot ' \\\n",
    "              'Int16 {} {}'.format(txmin, tymin, txmax, tymax, out_path_pcs_wet, out_path_gcs_wet)\n",
    "        run_cmd(cmd, logger)\n",
    "\n",
    "        if not is_valid_image(out_path_gcs_wet):\n",
    "            logger.error(\"compositing error for tile {} at wet season for twice\".format(tile_id))\n",
    "\n",
    "\n",
    "\n",
    "    # convert to Cloud-Optimized Geotiff          \n",
    "    cmd = 'gdal_translate -q {} {} -co COMPRESS=LZW -co TILED=YES'. format(out_path_gcs_wet, out_path_gcs_wet_TCI)\n",
    "    run_cmd(cmd, logger)\n",
    "\n",
    "    cmd = 'gdaladdo -q -r average {} 2 4'. format(out_path_gcs_wet_TCI)\n",
    "    run_cmd(cmd, logger)\n",
    "\n",
    "    cmd = 'gdal_translate -q {} {} -co COMPRESS=LZW -co COPY_SRC_OVERVIEWS=YES -co TILED=YES'. format(out_path_gcs_wet_TCI,\n",
    "                                                                                                      out_path_wet)\n",
    "    run_cmd(cmd, logger)\n",
    "\n",
    "\n",
    "    ############################################################\n",
    "    #             3.upload compositing image to s3             #\n",
    "    ############################################################\n",
    "    s3 = boto3.client('s3')\n",
    "    try:\n",
    "        s3.upload_file(out_path_dry, bucket, '{}/{}/OS/tile{}_{}_{}.tif'.format(prefix, output_prefix, tile_id, dry_lower_ordinal,\n",
    "                                                                                dry_upper_ordinal))\n",
    "    except ClientError as e:\n",
    "        logger.error(\"S3 uploading fails for tile{}_{}_{} : {}\".format(tile_id, dry_lower_ordinal, dry_upper_ordinal, e))\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        s3.upload_file(out_path_wet, bucket, '{}/{}/GS/tile{}_{}_{}.tif'.format(prefix, output_prefix, tile_id, wet_lower_ordinal,\n",
    "                                                                                wet_upper_ordinal))\n",
    "    except ClientError as e:\n",
    "        logger.error(\"S3 uploading fails for tile{}_{}_{}: {}\".format(tile_id, wet_lower_ordinal, wet_upper_ordinal, e))\n",
    "        raise\n",
    "\n",
    "    ##########################################################\n",
    "    #             delete local composte files                #\n",
    "    ##########################################################\n",
    "\n",
    "    # ARD folder\n",
    "    if bsave_ard is False:\n",
    "        # dry season\n",
    "        delete_file(out_path_gcs_dry, logger)\n",
    "        delete_file(out_path_pcs_dry, logger)\n",
    "        delete_file(out_path_gcs_dry_TCI, logger)\n",
    "        delete_file(out_path_dry, logger)\n",
    "\n",
    "        # wet season\n",
    "        delete_file(out_path_gcs_wet, logger)\n",
    "        delete_file(out_path_pcs_wet, logger)\n",
    "        delete_file(out_path_gcs_wet_TCI, logger)\n",
    "        delete_file(out_path_wet, logger)\n",
    "\n",
    "        # Try to delete the ard image folder\n",
    "        try:\n",
    "            shutil.rmtree(ard_folder)\n",
    "        except OSError as e:  # if failed, report it back to the user ##\n",
    "            logger.warning(\"Error for removing ARD folder {}: {} \".format(ard_folder, e.strerror))\n",
    "\n",
    "\n",
    "def ard_composition_execution(foc_img_catalog, foc_gpd_tile, tile_id, s3_bucket, prefix, img_fullpth_catalog, tmp_pth, compositing_exe_path,\n",
    "                              dry_lower_ordinal, dry_upper_ordinal, wet_lower_ordinal, wet_upper_ordinal, bsave_ard, output_prefix,\n",
    "                              res, logger, total_number):\n",
    "    \"\"\"\n",
    "    executing ARD and composite generation.\n",
    "    arg:\n",
    "        foc_img_catalog: a list recording all images for a focused tile id\n",
    "        foc_gpd_tile: geopandas object indicating the extent of a focused tile, using GCS system\n",
    "        tile_id: id of tile to be focused\n",
    "        s3_bucket: Name of the S3 bucket.\n",
    "        prefix: the prefix for tile_folder and img_folder\n",
    "        img_fullpth_catalog: a catalog for recording full uri path for each planet image\n",
    "        tmp_path: tmp path defining the path for storing temporal files\n",
    "        compositing_exe_path: directory for compositing exe\n",
    "        dry_lower_ordinal: lower bounds of ordinal days for dry season\n",
    "        dry_upper_ordinal: upper bounds of ordinal days for dry season\n",
    "        wet_lower_ordinal: lower bounds of ordinal days for wet season\n",
    "        wet_upper_ordinal: upper bounds of ordinal days for wet season\n",
    "        bsave_ard: if save ard images\n",
    "        output_prefix: prefix for composite in S3\n",
    "        res: resolution\n",
    "        logger: logging object\n",
    "        total_number: finished tile number for progress report\n",
    "    \"\"\"\n",
    "\n",
    "    # read proj and bounds from the first img of aoi\n",
    "    sample_img_nm = foc_img_catalog.iloc[0, 0]\n",
    "    tile_geojson, proj = get_geojson_pcs(s3_bucket, foc_gpd_tile, sample_img_nm, img_fullpth_catalog, logger)\n",
    "    bounds, (n_row, n_col) = get_extent(tile_geojson, res)\n",
    "\n",
    "    # generate ARD images\n",
    "    ard_generation(foc_img_catalog, img_fullpth_catalog, s3_bucket, tile_id, proj, bounds, n_row, n_col,\n",
    "                   tmp_pth, logger, dry_lower_ordinal, dry_upper_ordinal, wet_lower_ordinal, wet_upper_ordinal)\n",
    "\n",
    "    # compositing\n",
    "    try:\n",
    "        composite_generation(compositing_exe_path, s3_bucket, prefix, foc_gpd_tile, tile_id,\n",
    "                             os.path.join(tmp_pth, 'tile{}'.format(tile_id)), tmp_pth, logger, dry_lower_ordinal,\n",
    "                             dry_upper_ordinal, wet_lower_ordinal, wet_upper_ordinal, bsave_ard, output_prefix)\n",
    "    except (OSError, ClientError, subprocess.CalledProcessError) as e:\n",
    "        logger.error(\"Compositing failed for tile_id {} ({}))\".format(tile_id, datetime.now(timezone('US/Eastern'))\n",
    "                                                                      .strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    else:\n",
    "        logger.info(\"Progress: finished compositing for tile_id {} ({}))\".format(tile_id, datetime.now(timezone('US/Eastern'))\n",
    "                                                                                 .strftime('%Y-%m-%d %H:%M:%S')))\n",
    "\n",
    "    logger.info(\"Progress: the total finished tile number is {} ({}))\".format(total_number, datetime.now(tz)\n",
    "                          .strftime('%Y-%m-%d %H:%M:%S')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/composite/lib/python3.7/site-packages/ipykernel_launcher.py:137: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "config_filename = 'cvmapper_config_composite.yaml'\n",
    "tile_id = 487089\n",
    "aoi = None\n",
    "bsave_ard = False\n",
    "s3_bucket = 'activemapper'\n",
    "output_prefix = 'composite_sr'\n",
    "csv_pth = '/home/ubuntu/source/qtiles_priority.csv'\n",
    "\n",
    "# define res\n",
    "res = 3\n",
    "\n",
    "# define compositing ext\n",
    "compositing_exe_path = '/home/ubuntu/imager/C/AFMapTSComposite/bin/composite'\n",
    "\n",
    "# define log path\n",
    "log_path = '%s/log/planet_composite.log' % os.environ['HOME']\n",
    "logging.basicConfig(filename=log_path, filemode='w', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "tmp_pth = '/tmp'\n",
    "\n",
    "# parse mapper parameter from yaml\n",
    "params = parse_yaml_from_s3(s3_bucket, config_filename)['mapper']\n",
    "\n",
    "# read individual parameters\n",
    "prefix = params['prefix']\n",
    "\n",
    "# fetching a table linking  planet images name and tile id\n",
    "img_catalog_name = params['img_catalog_name']\n",
    "\n",
    "# fetching a table recording full pth of planet images\n",
    "img_catalog_pth = params['img_catalog_pth']\n",
    "\n",
    "# a geojson indicating the extent of each tile\n",
    "tiles_geojson_path = params['tile_geojson_path']\n",
    "\n",
    "# define lower and upper bounds for dry and wet season\n",
    "dry_lower_ordinal = params['dry_lower_ordinal'] # 2018/12/01\n",
    "dry_upper_ordinal = params['dry_upper_ordinal'] # 2019/02/28\n",
    "wet_lower_ordinal = params['wet_lower_ordinal'] # 2018/05/01\n",
    "wet_upper_ordinal = params['wet_upper_ordinal'] # 2018/09/30\n",
    "\n",
    "# time zone\n",
    "tz = timezone('US/Eastern')\n",
    "logger.info(\"Progress: starting a compositing task ({})\".format(datetime.now(tz).strftime('%Y-%m-%d %H:%M:%S')))\n",
    "\n",
    "# read a catalog for linking planet images and tile id\n",
    "img_catalog = parse_catalog_from_s3(s3_bucket, prefix, img_catalog_name)\n",
    "\n",
    "# read a catalog recording full path for planet images\n",
    "img_fullpth_catalog = parse_catalog_from_s3(s3_bucket, prefix, img_catalog_pth)\n",
    "\n",
    "# read a geopandas object for tile geojson\n",
    "uri_tile = \"s3://{}/{}/{}\".format(s3_bucket, prefix, tiles_geojson_path)\n",
    "gpd_tile = gpd.read_file(uri_tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ard_folder = os.path.join(tmp_pth, 'tile{}'.format(tile_id))\n",
    "\n",
    "foc_img_catalog = img_catalog.loc[img_catalog['tile'] == int(tile_id)]\n",
    "sample_img_nm = foc_img_catalog.iloc[0, 0]\n",
    "foc_gpd_tile = gpd_tile[gpd_tile['tile'] == int(tile_id)]\n",
    "tile_geojson, proj = get_geojson_pcs(s3_bucket, foc_gpd_tile, sample_img_nm, img_fullpth_catalog, logger)\n",
    "bounds, (n_row, n_col) = get_extent(tile_geojson, res)\n",
    "\n",
    "# fetch tile info, which will be used to crop intermediate compositing image\n",
    "extent_geojson_gcs = mapping(foc_gpd_tile['geometry'])\n",
    "txmin = extent_geojson_gcs['bbox'][0]\n",
    "txmax = extent_geojson_gcs['bbox'][2]\n",
    "tymin = extent_geojson_gcs['bbox'][1]\n",
    "tymax = extent_geojson_gcs['bbox'][3]\n",
    "\n",
    "#######################################################\n",
    "#           1. begin compositing dryseason            #\n",
    "#######################################################\n",
    "cmd = [compositing_exe_path, ard_folder, tmp_pth, str(tile_id), str(dry_lower_ordinal), str(dry_upper_ordinal)]\n",
    "# run composite exe\n",
    "try:\n",
    "    p = subprocess.check_output(cmd, stderr=subprocess.STDOUT)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    logger.error(\"compositing error for tile {} at dry season: {}\".format(tile_id, e))\n",
    "    raise\n",
    "\n",
    "# reproject and crop compositing image to align with GCS tile system\n",
    "out_path_pcs_dry = os.path.join(tmp_pth, 'tile{}_{}_{}_pcs.tif'.format(tile_id, dry_lower_ordinal, dry_upper_ordinal))\n",
    "out_path_gcs_dry = os.path.join(tmp_pth, 'tile{}_{}_{}_gcs.tif'.format(tile_id, dry_lower_ordinal, dry_upper_ordinal))\n",
    "out_path_gcs_dry_TCI = os.path.join(tmp_pth, 'tile{}_{}_{}_gcs_TCI.tif'.format(tile_id, dry_lower_ordinal, dry_upper_ordinal))\n",
    "out_path_dry = os.path.join(tmp_pth, 'tile{}_{}_{}.tif'.format(tile_id, dry_lower_ordinal, dry_upper_ordinal))\n",
    "\n",
    "\n",
    "# here call gdalwarp directly instead of gdal.warp, cause unexpected bug for gdal.warp\n",
    "# out_img = gdal.Warp(out_path_gcs_dry, img, outputBounds=[txmin, tymin, txmax, tymax], resampleAlg=gdal.GRA_Bilinear, width=2000,\n",
    "                    # height=2000, dstNodata=-9999,outputType=gdal.GDT_Int16, dstSRS='EPSG:4326')\n",
    "cmd = 'gdalwarp -q -overwrite -t_srs EPSG:4326 -te {} {} {} {} -r bilinear -ts 2000 2000 -srcnodata -9999 -dstnodata -9999 -ot ' \\\n",
    "      'Int16 {} {}'.format(txmin, tymin, txmax, tymax, out_path_pcs_dry, out_path_gcs_dry)\n",
    "\n",
    "run_cmd(cmd, logger)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "#                 convert to Cloud-Optimized Geotiff                                  #\n",
    "#        (source: https://trac.osgeo.org/gdal/wiki/CloudOptimizedGeoTIFF)             #\n",
    "#   why create a memory driver filer, not directly created:                           #\n",
    "#   the problem is that this will give an error in the COG format, because the        # \n",
    "#   pyramids were created after the tiling.                                           #\n",
    "#######################################################################################\n",
    "cmd = 'gdal_translate -q {} {} -co COMPRESS=LZW -co TILED=YES'. format(out_path_gcs_dry, out_path_gcs_dry_TCI)\n",
    "run_cmd(cmd, logger)\n",
    "\n",
    "cmd = 'gdaladdo -q -r average {} 2 4'. format(out_path_gcs_dry_TCI)\n",
    "run_cmd(cmd, logger)\n",
    "\n",
    "cmd = 'gdal_translate -q {} {} -co COMPRESS=LZW -co COPY_SRC_OVERVIEWS=YES -co TILED=YES'.format(out_path_gcs_dry_TCI,\n",
    "                                                                                                 out_path_dry)\n",
    "run_cmd(cmd, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a316298c796d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# run composite exe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compositing error for tile {} at wet season: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/composite/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0;32m--> 395\u001b[0;31m                **kwargs).stdout\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/composite/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/composite/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    924\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "#            2. begin compositing wet season            #\n",
    "#########################################################\n",
    "cmd = [compositing_exe_path, ard_folder,tmp_pth, str(tile_id), str(wet_lower_ordinal), str(wet_upper_ordinal)]\n",
    "# run composite exe\n",
    "try:\n",
    "    p = subprocess.check_output(cmd, stderr=subprocess.STDOUT)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    logger.error(\"compositing error for tile {} at wet season: {}\".format(tile_id, e))\n",
    "    raise\n",
    "\n",
    "# reproject and crop compositing image to align with GCS tile system         \n",
    "out_path_pcs_wet = os.path.join(tmp_pth, 'tile{}_{}_{}_pcs.tif'.format(tile_id, wet_lower_ordinal, wet_upper_ordinal))\n",
    "out_path_gcs_wet = os.path.join(tmp_pth, 'tile{}_{}_{}_gcs.tif'.format(tile_id, wet_lower_ordinal, wet_upper_ordinal))\n",
    "out_path_gcs_wet_TCI = os.path.join(tmp_pth, 'tile{}_{}_{}_gcs_TCI.tif'.format(tile_id, wet_lower_ordinal, wet_upper_ordinal))\n",
    "out_path_wet = os.path.join(tmp_pth, 'tile{}_{}_{}.tif'.format(tile_id, wet_lower_ordinal, wet_upper_ordinal))\n",
    "\n",
    "\n",
    "# img = gdal.Open(out_path_pcs_wet)\n",
    "# if img is None:\n",
    "    # logger.error(\"couldn't find pcs-based compositing result for tile {}\".format(tile_id))\n",
    "    # return\n",
    "\n",
    "# out_img = gdal.Warp(out_path_gcs_wet, img, outputBounds=[txmin, tymin, txmax, tymax], resampleAlg=gdal.GRA_Bilinear, width=2000,\n",
    "                    #height=2000, dstNodata=-9999, xRes=0.05/2000, yRes=0.05/2000, outputType=gdal.GDT_Int16, dstSRS='EPSG:4326')\n",
    "\n",
    "cmd = 'gdalwarp -q -overwrite -t_srs EPSG:4326 -te {} {} {} {} -r bilinear -ts 2000 2000 -srcnodata -9999 -dstnodata -9999 -ot ' \\\n",
    "      'Int16 {} {}'.format(txmin, tymin, txmax, tymax, out_path_pcs_wet, out_path_gcs_wet)\n",
    "run_cmd(cmd, logger)\n",
    "\n",
    "# check if the composite image is empty\n",
    "if not is_valid_image(out_path_gcs_wet):\n",
    "    logger.error(\"Composition fails for the first time for tile{}_{}_{}\".format(tile_id, wet_lower_ordinal, wet_upper_ordinal))\n",
    "    cmd = [compositing_exe_path, ard_folder, tmp_pth, str(tile_id), str(wet_lower_ordinal), str(wet_upper_ordinal)]\n",
    "    # run composite exe\n",
    "    p = subprocess.check_output(cmd, stderr=subprocess.STDOUT)\n",
    "\n",
    "    cmd = 'gdalwarp -q -overwrite -t_srs EPSG:4326 -te {} {} {} {} -r bilinear -ts 2000 2000 -srcnodata -9999 -dstnodata -9999 -ot ' \\\n",
    "          'Int16 {} {}'.format(txmin, tymin, txmax, tymax, out_path_pcs_wet, out_path_gcs_wet)\n",
    "    run_cmd(cmd, logger)\n",
    "\n",
    "    if not is_valid_image(out_path_gcs_wet):\n",
    "        logger.error(\"compositing error for tile {} at wet season for twice\".format(tile_id))\n",
    "\n",
    "\n",
    "\n",
    "# convert to Cloud-Optimized Geotiff          \n",
    "cmd = 'gdal_translate -q {} {} -co COMPRESS=LZW -co TILED=YES'. format(out_path_gcs_wet, out_path_gcs_wet_TCI)\n",
    "run_cmd(cmd, logger)\n",
    "\n",
    "cmd = 'gdaladdo -q -r average {} 2 4'. format(out_path_gcs_wet_TCI)\n",
    "run_cmd(cmd, logger)\n",
    "\n",
    "cmd = 'gdal_translate -q {} {} -co COMPRESS=LZW -co COPY_SRC_OVERVIEWS=YES -co TILED=YES'. format(out_path_gcs_wet_TCI,\n",
    "                                                                                                  out_path_wet)\n",
    "run_cmd(cmd, logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path_pcs_dry = os.path.join(tmp_pth, 'tile{}_{}_{}_pcs.tif'.format(tile_id, dry_lower_ordinal, dry_upper_ordinal))\n",
    "out_path_gcs_dry = os.path.join(tmp_pth, 'tile{}_{}_{}_gcs.tif'.format(tile_id, dry_lower_ordinal, dry_upper_ordinal))\n",
    "out_path_gcs_dry_TCI = os.path.join(tmp_pth, 'tile{}_{}_{}_gcs_TCI.tif'.format(tile_id, dry_lower_ordinal, dry_upper_ordinal))\n",
    "out_path_dry = os.path.join(tmp_pth, 'tile{}_{}_{}.tif'.format(tile_id, dry_lower_ordinal, dry_upper_ordinal))\n",
    "out_path_wet = os.path.join(tmp_pth, 'tile{}_{}_{}.tif'.format(tile_id, wet_lower_ordinal, wet_upper_ordinal))\n",
    "ds = gdal.Open(out_path_pcs_dry)\n",
    "rasterArray = np.array(ds.GetRasterBand(1).ReadAsArray())\n",
    "unique_val = np.unique(rasterArray) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_valid_image(out_path_pcs_dry)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:composite] *",
   "language": "python",
   "name": "conda-env-composite-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
